[["index.html", "Data analysis with R and Oracle Chapter 1 Who is this book for? 1.1 What will the book cover? 1.2 What does the book not cover?", " Data analysis with R and Oracle Henning Holgersen 2021-04-22 Chapter 1 Who is this book for? You are probably employed as an analyst at some company, where the IT department has the production data in an Oracle database. You are familiar with R, and have figured out that loading the data you need to analyze from the database to R is the most effective way for you to do your job. 1.1 What will the book cover? This book will go through systems requirements for connecting to Oracle from R, how to retrieve data from the database, and how to write custom queries. It outlines what types of tasks the database excels at, and in which use cases the processing is best left to R. 1.2 What does the book not cover? A lot of great work has been done in recent years on database connectivity from R. Perhaps most notably, the dbplyr package does an excellent job at letting you as an analyst write tidy R code, and have the queries executed by the database while you remain blissfully ignorant of what happens under the hood. If you are proficient in R and don't have any interest in learning SQL, skipping this book and learning dbplyr is probably a better use of your time. "],["intro.html", "Chapter 2 Introduction 2.1 What analysts need", " Chapter 2 Introduction Are you past the point where your source data is an excel spreadsheet? Has the IT department granted you access to their database? Are you expected to rummage through way more data than you can hold in memory? Databases used to be great at little more than retrieving a given record at speed, and making sure that reads and writes happened consistenly and in order. Boring as they may sound, such features made sure that your balance was returned to you when you went to the ATM, and that you couldn't withdraw the same money twice in rapid succession. Quick information retrieval and data consistency is still the bread and butter of databases, but the marmelade is a rapidly evolving set of powerful features aimed at analyzing and aggregating vast amounts of data. This book will guide you through some of the useful features that are available in modern databases, focusing on the possibilities in Oracle 12c. Oracle is a feature-rich database, and widely used database in corporate environments. And it is in corporate production environments most of todays data analysis needs are situated. The examples could just as easily have been about PostgreSQL, which is also very powerful, has some amazing features and extensions, and unlike Oracle is open-source. In contrast, Oracle is widely (and wildly!) disdained in the open-source community, not without reason. Coupling commercial Oracle with open-source R could seem like a joke, but it is not. To that point, both Oracle and Microsoft are making great efforts to integrate R with their databases. Luckily, many of the great features in Oracle are available in Postgres and other database systems as well. The examples will be based on Oracle, and we will not cover functions available in Postgres but not Oracle, but there is a large overlap. And I will make my best efforts to note which features are available verbatim in other databases, which have close analogues, and which are exclusive to Oracle. 2.1 What analysts need The basic outline of an analysis is ususally something like this: You wake up in the middle of the night, with a bright and shiny idea. Or maybe an epiphany hits you when you are biking home from work. Wherever it strikes you, all of a sudden you have a theory or a glowing need to check something in your data. The first step is selecting (filtering), the tables, rows and columns you want. This might be tedious in itself, and require you to track down some near-retiree in accounting just to discover that the column you thought was a godsend is actually 85% missing and of no real use to you, or that the documentation for half of the categorical variables you want to use is nowhere to be found. Discouraged, this might derail your entire analysis. Even though &quot;it can't be done&quot; is a valid result, it is not a satisfying one. But maybe the shortcomings in data quality and documentation are only a speed bump. Blissfully, you conclude that the basic information needed to answer your question is present, and delude yourself to think that the rest is trivial. Simply a few lines of code, and amazing new insight will be shining from your lacklustre office monitor. Alas, although you now know the mission should be possible it is far from accomplished. A lot of selecting, filtering, reshaping and cleaning is still ahead of you, and as any programmer knows, the little things can trip you just as bad as the big things. Having understood the source data, you know which columns to select and which conditions to filter the data on. On a good day this is two lines of code, but if you need to filter data on some less trivial conditions it might take days simply to carve out the necessary logic to subset your data correctly, and another day to verify that it did indeed work as intended. The next hurdle is structuring the data. Although data stored in databases usually follow a clear structure, this is often not the structure you need to have in order to complete your analyis. Like earlier, this could be as simple as joining a couple of tables or it could turn out to be a bottomless rabbithole of new things to learn - with a lot of dead ends and failed attempts on the way. Still you are not done, even though you had hoped you were on the home straight. The structuring and aggregations have probably revealed something new about your data, outliers that had gone unnoticed before or invalid combinations of values. More data cleaning is necessary, and a grand combination of heuristic rules, outlier detection thresholds and maybe even imputation rules are applied in order to cover up the horrors that were exposed by your adventures in data consolidation. By now it has been a week since you got your bright idea, you have worked on it constantly, and still you have nothing to show for it. Your boss is losing confidence in you, probably aided by the phone call he got from accounting concerning all the weird data questions you were bothering their people with. Go home. Call it a week. Come back rested on monday. Finally though, the grunt work is over. The data makes sense, and you can feed it into the algorithms and visualizations you were dreaming about. After two error messages due to incorrect data types, all the columns have been converted and the algorithm churns away. For once, you made the computer sweat, not the other way around. You whip together a simple choropleth map, and send it to your boss. After all that toil, the thing that makes him excited is a few pretty colors that were created in five lines of code. If you could only persuade IT to let you have a Shiny server on the company network, he would probably be beside himself. Nah, a choropleth will have to do for now. The tediousness of analysis is unescapable, but a good understanding of databases can help the situation: Understanding the thinking behind relational database architecture can help you understand how the source data is structured, ask the right questions, and keep you on the good side of the sysadmins. Initial exploratory analysis might be done directly against the source data using a SQL client, speeding up the question-answer process significantly by giving you access to all the data in the blink of an eye. Even though you can do initial exploration in an SQL client, you will probably need several attempts at extracting the source data from the database. A seamless and fast workflow to alter queries and load data from the database to R will speed up analysis. Some aggregations, joins and calculations are better done in the database. For example: When a lot of rows simply need to be aggregated, before analysis can begin. Rather than transfering and loading 5 gigabytes of data into R, it can be a lot quicker to sum them in SQL and load maybe 100 MB instead. And if the data is even bigger, it might not even be possible to load it all into R. Aggregating it in the database is the convenient alternative to some slow, complicated mapreduce-like aggregation in R. Doing joins in the database can allow you to filter out unneeded data instead of first loading it in R and filtering it thereafter. The seemingly random differences in available functions between R and SQL can sometimes work in SQL's advantage. Some typical business functions are easily avaiable in SQL, and some more novel and complex functions are only available in SQL. Depending on your computer and your database, some calculations are simply faster in SQL. All this allows you to explore your data faster, iterate quickly, spend less time waiting for the computer to finish, and create more novel analysis by drawing on a richer set of analytical functions. "],["system-setup.html", "Chapter 3 System setup 3.1 Follow along with the BigData Lite VM or XE18 on Docker 3.2 A production environment 3.3 Connecting to a database from R", " Chapter 3 System setup 3.1 Follow along with the BigData Lite VM or XE18 on Docker I started writing this book on the Oracle Bigdata Lite VM. This is a free virtual machine you can download from Oracle, and contains both an Oracle 12c installation, drivers, Rstudio Server, Hadoop, hive, and a whole host of other &quot;Big data&quot; related applications you can use to explore and learn. After a while, the Oracle 18c Express Edition (XE) came out, which is a free version of Oracle 18c which features all the same bells and whistles as the production grade database, but has restricted resources in terms of CPU, RAM and disk. For our use though, this database is perfect, and that's probably exactly what Oracle intended - let developers learn and experiment for free, while companies have to shell out for their products. I am running the XE version in a docker container, and linked it to an Rstudio container. This way, you can follow along with this book simply using two docker containers. Sadly, Oracle doesn't provide the XE container image directly, so you need to download the database install file from the Oracle website (requires a free account), and place it in a designated forlder for build to use. The same is true for the Oracle drivers which is required for the rstudio container. The build scripts for the containers can be found at https://github.com/radbrt/xestudio. 3.2 A production environment This book is not going to cover how to set up a database. If you just want to learn, the Bigdata Lite VM will be a great resource. If you need to start storing production data in a database, you should probably leave it to dedicated database administrators, or use a cloud service (database-as-a-service) that leaves the management to the provider. With the database management is taken care of, you still might have to install Rstudio and Oracle drivers on your own. And while you are at it, install SQL Workbench. This interactive graphical SQL-client has a lot of advanced features, provides a very responsive way to execute queries when testing and exploring, and is a perfect companion to Rstudio. 3.2.1 Installing drivers 3.2.2 Installing Rstudio Rstudio is available from https://rstudio.com. There is a free version distributed with the AGPL license, as well as a commercial version for those who need. Rstudio develops and supports a wide array of libraries, addons and apps that are available for free, so if you are in a commercial environment this is an added bonus to paying for the commercial license and commercial support. Rstudio is available both as a desktop version and a server with web-interface. The two versions have a very similar look and feel, so choosing which one to install is mostly a question of which one is most practical for your use case. The desktop version supports most operating systems including the major linux distributions, while the server is supported on the major linux distributions. 3.2.3 Installing SQL Workbench The workbench is a Java application, and supports Windows, Mac and major linux distributions. Simply download and install from https://www.mysql.com/products/workbench/. Connecting to the database is fairly simple from the dialog box. The server hostname should be familiar to you (or ask the system administrator), as with the username/password. Oracle uses port 1521 by default, and this is also the default port number selected in the dialog. Depending on the server setup you either have to specify a service ID (SID), or a Service name. Again, your sysadmin should be able to help you with these details. 3.3 Connecting to a database from R There are multiple libraries available in R that lets you connect to a database, run queries, and transfer data. For oracle users, the most convenient one might be ROracle. install.packages(&quot;ROracle&quot;) In order to connect to Oracle using this library, you need your connection details, and to have installed the Oracle drivers on your system. When installed correctly the drivers are registered with the operating system, so that you do not need to refer to the exact location of the driver. library(ROracle) drv &lt;- dbDriver(&quot;Oracle&quot;) host &lt;- &quot;localhost&quot; port &lt;- 1521 sid &lt;- &quot;orcl&quot; connect.string &lt;- paste( &quot;(DESCRIPTION=&quot;, &quot;(ADDRESS=(PROTOCOL=tcp)(HOST=&quot;, host, &quot;)(PORT=&quot;, port, &quot;))&quot;, &quot;(CONNECT_DATA=(SID=&quot;, sid, &quot;)))&quot;, sep = &quot;&quot;) connection &lt;- dbConnect(drv, username=&quot;moviedemo&quot;, password=&quot;welcome1&quot;, dbname=connect.string) tickercon &lt;- dbConnect(drv, username=&quot;pmuser&quot;, password=&quot;pmuser&quot;, dbname=connect.string) This should leave you with two new objects called connection and tickercon, both instances of class OraConnection, and represent connections to two different databases that ship with the BigdataLite VM. Password security If you cringe when you look at the hardcoded password above, good on you. In this example, with a test-database, it doesn't matter. But when you set up a workflow in production, you should shun hardcoded, plaintext passwords. Rstudio has a great resource on this, see http://db.rstudio.com/best-practices/managing-credentials/. The fastest solution is to replace the hardcoded password with rstudioapi::askForPassword(&quot;Enter password&quot;), and you will be prompted for the password. "],["a-relational-state-of-mind.html", "Chapter 4 A relational state of mind 4.1 An illustrated example with movie-data", " Chapter 4 A relational state of mind Compared to the data you might be used to analyze in school, database tables are usually written to regularly, so that the amount of data in the table grows. Perhaps it is also designed to have data regularly deleted or updated, as data stops being valid, relevant or correct. This need to write governs one of the most important principles of table structure: Adding more data should result in more rows, never more columns. Let's illustrate this with an example: A major news website lets users sign up and choose subjects they are interested in. Since any user can choose an arbitrary number of subjects, there is no way for the database designers to anticipate the maximum number subjects that a user will choose and create that number of columns. Doubling down and creating one column for each topic is probably not a good idea either, as editors are likely to add and remove topics as they see fit. Instead, they would probably choose to create a new table, containing only user-IDs and topics. Here, a user would have one row for every topic she liked, and the developers won't have to constantly alter the database tables. Splitting data into multiple tables is also the solution to another database principle: Don't duplicate information. Say for example you are storing information about employees and their employers. When it's time to analyze the data, you probably want a single table containing columns such as employee ID, employee title, salary and tenure, along with columns such as number of employees in company, market value, CEO pay, etc. Employer information would be repeated for every employee in the same company. From a database perspective, that repetition would be unfortunate. Not only does it take up additional space, but it would enable inconsistencies in the data. The same company could accidentaly be stored with two different market values, and it would be impossible for us to know which is correct. Again, the solution is to split employer data into a separate table, and only store an employer-ID in the employee table. When integrating different types of data, these principles can quickly manifest themselves in a giant maze of tables with not just one but many links between them. It is often useful to visualize entire database structure in an ER-diagram in order to get a high-level overview of the tables, columns and connections between the tables. The IDs connecting the tables might be entirely fictional number sequences, and in database jargon these ID columns are referred to as keys and foreign keys. In our example, the employer ID stored in the employee table would be a foreign key because it refers to a column in the employer table that is designed to identify a single employer. In a random dataset created for analysis purposes, defining what makes a row uniquely identifiable might be completely irrelevant. For database developers, it is second nature to require a unique identifier for every row in a table. If the data does not have one, a new ID column might be created simply to have that uniqueness. The IDs would be a meaningless sequence of numbers, but it would be a unique identifier and referred to as a primary key because it singlehandedly identifies every row in the table uniquely. 4.1 An illustrated example with movie-data The VM comes with a collection of tables relating to movies. One table describes the movies themselves, another table describes actors etc, and a third table contains the connections between the actors and the movies. Here are the crew and movie_crew tables respectively: This type of data is referred to as &quot;Normalized data&quot;. There are many versions of normalization and experts may have bitter arguments about the details, but the most important term to remember might be &quot;third normal form&quot;, which (despite its ridiculous name) refers to what most people would say is a &quot;common sense&quot; split of the data in order to avoid data duplication (more on the third normal form on Wikipedia: https://en.wikipedia.org/wiki/Third_normal_form). The reason the tables are split like this, are of course that one actor can appear in several movies. The relationship between the two tables can be illustrated in an ER (entity relation) diagram: Normalized data The three lines going from movie_crew converging to a single arrow pointing to crew indicates that several rows in the movie_crew table can point to a single row in the crew table - consistent with the fact that one actor or director can be involved in several movies. If we were to store all this information in a single table, we would have to repeat people's names as they appear in several movies. Such information duplication is both expensive (in terms of storining duplicate information) and risky in the sense that it becomes more difficult to know if two names refer to the same person. When reading and analyzing data, so-called &quot;denormalized&quot; data might be more useful. Denormalization can easily be achieved by joining the data tables (more on this in chapter 6). Denormalized data "],["loading-data.html", "Chapter 5 Loading our data", " Chapter 5 Loading our data If you are using the VM, you will already have available some data to play around with. However, we are going to use a different dataset for most of our analysis. Kaggle hosts a dataset containing data from an ecommerce-site, which is both interesting and quite conductive to learning advanced SQL. In order to download the data you need a Kaggle account, but once that is set up, you can download the data from https://www.kaggle.com/carrie1/ecommerce-data/downloads/ecommerce-data.zip. In order to load it into the SQL database, we will first load it into R as a regular dataframe and then write it to a table. trx &lt;- read.csv(&quot;data/online_transactions.csv&quot;, stringsAsFactors = F) trx$InvoiceDate &lt;- strptime(x = trx$InvoiceDate, format = &quot;%m/%d/%Y %H:%M&quot;) names(trx) &lt;- lapply(names(trx), toupper) The InvoiceDate is a timestamp but R reads it as a string instead of a date, so we needed to convert it with the strptime command. We also convert all variable names to uppercase, as Oracle seems strangely sensitive to this (SQL is not usually case-sensitive). Writing the dataframe to a new table in the moviedemo database can be done with dbWriteTable, a function in the ROracle library that needs only three arguments: the database connection object, a new table name, and the name of our dataframe. dbWriteTable(connection, &quot;TRX0&quot;, trx) Unfortunately, the date format is lost from R to Oracle so we need to do a little data transformation in SQL as well. We do want the date field as an actual date, even though dates can be a hassle (like now), they prove very valuable for anyone who invests some time in learning about them. This statement could have been executed from R, but using the SQL Workbench is probably just as nice. CREATE TABLE trx AS ( SELECT INVOICENO, STOCKCODE, DESCRIPTION, QUANTITY, TO_DATE(INVOICEDATE, &#39;YYYY-MM-DD HH24:MI:SS&#39;) AS INVOICEDATE, UNITPRICE, CUSTOMERID, COUNTRY FROM moviedemo.trx0 ) "],["basic-sql.html", "Chapter 6 Basic SQL 6.1 SELECT FROM WHERE? Starting our exploration 6.2 More ways to JOIN tables 6.3 A few common gotchas", " Chapter 6 Basic SQL The simplest queries are only a line or two of code, containing two or three keywords. All queries that read data from a table follows this structure: First, the keyword SELECT, followed by the variables (consistently referred to as columns in database jargon) you wish to select. Second, the keyword FROM followed by the name of the table or the view you want to select from. A database table is the analogous to a dataframe in R. Details differ, but it is a rectangular sheet of data consisting of rows and columns/variables. Unlike in normal R dataframes there is no inherent row index in SQL tables, but most database tables have an index created explicitly in order to ensure data integrety. You might want to subset the rows, not just the columns. This is done with the WHERE keyword, followed by whatever conditions you like. Unlike R, SQL uses single equal-signs to check for equality. SQL is case-insensitive, but a certain coding standard has evolved. 6.1 SELECT FROM WHERE? Starting our exploration We will be working with the movie table that comes with the BigData Lite VM. This table contains the movie title, year, budget, revenue and plot summary from some fairly recent movies. Usually you would probably do the initial exploration in a standalone SQL client, but if you want to explore the data with R, perhaps to give some basic description to your readers, it can easily be done. 6.1.1 A quick glance at our data No matter how advanced, most data analysis starts with a simple look at the data just to get a feel for some real values. Let's select the first 10 rows from the table. SELECT * FROM trx FETCH FIRST 10 ROWS ONLY Table 6.1: Displaying records 1 - 10 INVOICENO STOCKCODE DESCRIPTION QUANTITY INVOICEDATE UNITPRICE CUSTOMERID COUNTRY 536384 22189 CREAM HEART CARD HOLDER 4 2010-12-01 09:53:00 4.0 18074 United Kingdom 536384 22427 ENAMEL FLOWER JUG CREAM 3 2010-12-01 09:53:00 6.0 18074 United Kingdom 536384 22428 ENAMEL FIRE BUCKET CREAM 6 2010-12-01 09:53:00 7.0 18074 United Kingdom 536384 22424 ENAMEL BREAD BIN CREAM 8 2010-12-01 09:53:00 10.9 18074 United Kingdom 536385 22783 SET 3 WICKER OVAL BASKETS W LIDS 1 2010-12-01 09:56:00 19.9 17420 United Kingdom 536385 22961 JAM MAKING SET PRINTED 12 2010-12-01 09:56:00 1.4 17420 United Kingdom 536385 22960 JAM MAKING SET WITH JARS 6 2010-12-01 09:56:00 4.2 17420 United Kingdom 536385 22663 JUMBO BAG DOLLY GIRL DESIGN 10 2010-12-01 09:56:00 1.9 17420 United Kingdom 536385 85049A TRADITIONAL CHRISTMAS RIBBONS 12 2010-12-01 09:56:00 1.2 17420 United Kingdom 536385 22168 ORGANISER WOOD ANTIQUE WHITE 2 2010-12-01 09:56:00 8.5 17420 United Kingdom InvoiceNo, StockCode and CustomerID seems to be random ID numbers, which will be useful later on. First, let's note that the description is a freetext column, there is no column for total salesprice, but this is easily calculated from UnitPrice and Quantity. Note that FETCH FIRST 10 ROWS ONLY is Oracle-specific, other databases have different syntaxes, e.g. LIMIT 10 in MySQL. Since we have customer IDs, we might be able to track a single customer over several purchases. To check this, we can select only rows from a single customer. SELECT * FROM trx WHERE CUSTOMERID=&#39;13058&#39; Table 6.2: 8 records INVOICENO STOCKCODE DESCRIPTION QUANTITY INVOICEDATE UNITPRICE CUSTOMERID COUNTRY 559636 47590B PINK HAPPY BIRTHDAY BUNTING 3 2011-07-11 12:01:00 5.5 13058 United Kingdom 559636 47590A BLUE HAPPY BIRTHDAY BUNTING 3 2011-07-11 12:01:00 5.5 13058 United Kingdom 559636 23298 SPOTTY BUNTING 3 2011-07-11 12:01:00 5.0 13058 United Kingdom 570832 47590A BLUE HAPPY BIRTHDAY BUNTING 6 2011-10-12 13:28:00 5.5 13058 United Kingdom 570832 47590B PINK HAPPY BIRTHDAY BUNTING 6 2011-10-12 13:28:00 5.5 13058 United Kingdom 570832 23313 VINTAGE CHRISTMAS BUNTING 5 2011-10-12 13:28:00 5.0 13058 United Kingdom 576539 23313 VINTAGE CHRISTMAS BUNTING 25 2011-11-15 12:43:00 5.0 13058 United Kingdom 576539 22776 SWEETHEART 3 TIER CAKE STAND 1 2011-11-15 12:43:00 9.9 13058 United Kingdom Indeed, we see this customer returning on three different dates between July and November 2011. You might have noticed that we have no information about the denomination for either the quantity or price, which is actually surprisingly common in real life. We suspect that the dataset comes from the US, which would make dollar the right currency. Amount-wise this makes sense, but it might be british pounds, or a combination. All the quantities are whole numbers, and the product names suggests that these are separate items, as opposed to kilos, meters, liters or some other more esoteric continuous measurement. 6.1.2 Aggregating your results First off, let's check the total sales at this shop. SELECT SUM(unitprice*quantity) AS totalSales FROM trx Table 6.3: 1 records TOTALSALES 9747748 The shop has sold for almost $10 millon, which is a good amount. But this triggers even more questions. How many units have it sold, and over which time period is this? This is where the date format pays off. We can simply subtract the maximum date from the minimum date in the table, and Oracle returns the number of days inbetween. Note that since we are dealing with timestamps, we are returned a decimal number which we can just round off. SELECT SUM(quantity) AS units_sold, SUM(unitprice*quantity) AS totalSales, ROUND(MAX(INVOICEDATE)-MIN(INVOICEDATE)) AS timeperiod, ROUND(SUM(unitprice*quantity)/SUM(quantity), 2) AS avg_price FROM trx Table 6.4: 1 records UNITS_SOLD TOTALSALES TIMEPERIOD AVG_PRICE 5176450 9747748 373 1.9 A lot is happening here, so we will go through each column individually: SUM(quantity) AS units_sold returns the sum of the quantity column, and uses the AS keyword to give the resulting column a new name units_sold. SUM(unitprice*quantity) AS totalSales multiplies the columns unitprice and quantity, and sums the product. The result is given the new column name totalSales. ROUND(MAX(INVOICEDATE)-MIN(INVOICEDATE)) AS timeperiod subtracts the latest (e.g. maximum) invoice date from the first (e.g. minimum) invoice date. The result is the number of days between the two dates, but since these columns are actually timestamps we are presented with a long decimal number which we round off using the ROUND() function. ROUND(SUM(unitprice*quantity)/SUM(quantity), 2) AS avg_price divides the total sales by the total units sold, which gives us the average unit price. To avoid overly long decimal numbers, we round it off to two decimal points. Lastly, let's just check the total number of rows in the table, using the COUNT(*) function. SELECT COUNT(1) AS number_of_rows FROM trx Table 6.5: 1 records NUMBER_OF_ROWS 541909 The number of rows is fairly unimportant from a business perspective, as it neither reflects the number of units sold or the number of transactions. The table also contains a Country column, which we have seen contains United Kingdom and we suspect contains the United States. Let's check how many countries there are, and the sales in each. In fact, we can reuse and extend the large query above to get statistics per country. SELECT country, SUM(quantity) AS units_sold, SUM(unitprice*quantity) AS totalSales, ROUND(MAX(INVOICEDATE)-MIN(INVOICEDATE)) AS timeperiod, ROUND(SUM(unitprice*quantity)/SUM(quantity), 2) AS avg_price FROM trx GROUP BY country ORDER BY 2 DESC Table 6.6: Displaying records 1 - 10 COUNTRY UNITS_SOLD TOTALSALES TIMEPERIOD AVG_PRICE United Kingdom 4263829 8187806 373 1.9 Netherlands 200128 284662 372 1.4 EIRE 142637 263277 372 1.9 Germany 117448 221698 373 1.9 France 110480 197404 373 1.8 Australia 83653 137077 358 1.6 Sweden 35637 36596 356 1.0 Switzerland 30325 56385 361 1.9 Spain 26824 54775 369 2.0 Japan 25218 35341 366 1.4 Note the final line, ORDER BY 2 DESC. This sorts the output data by the second column, in descending order. This way, the countries with the largest sales appear first. It is possible to order by multiple columns (we could say ORDER BY 2 DESC, country) to make sure any ties are sorted alphabetically by country. Passing column numbers instead of column nicknames is a nice shortcut, but we could just as well have written ORDER BY units_sold DESC. The ORDER BY clause is one of the few commands in Oracle that accepts the nicknames we create in the SELECT statement. This is indeed a nice overview, and properly sorted. Before we continue though, let's improve our understanding further with some visualizations. In order to visualize, we have to first load the data into an R data frame. We will show how to pass a query into a data frame using the ROracle library. This process consists of passing a query (and a database connection) into the function dbSendQuery. This, however, does not return the data. In order to return the data set, you have to run fetch on the result of dbSendQuery. It is this resulting data frame that we will be plotting, using ggplot. dfq &lt;- dbSendQuery(con, &quot; SELECT country, SUM(quantity*unitprice) AS sales_total FROM trx WHERE country IN( SELECT country FROM trx GROUP BY country ORDER BY COUNT(*) desc FETCH FIRST 4 ROWS ONLY ) GROUP BY COUNTRY, INVOICENO, CUSTOMERID &quot;) dfd2 &lt;- fetch(dfq) library(ggplot2) library(dplyr) dfd2 %&gt;% ggplot(aes(COUNTRY, SALES_TOTAL)) + geom_jitter(alpha=0.08, stroke=0) + scale_y_log10() ## Warning in scale$trans$trans(x): NaNs produced ## Warning: Removed 3661 rows containing missing values (geom_point). This figure plots the different reciept totals, for the four biggest markets of the store. We can see that we are dealing with totals centered in the 100 - 10 000 dollar range (1e+02 to 1e+04), suggesting that this store is probably into wholesale. The plot (or rather the warning messages) shows us something else as well: A good portion of the values are removed because the log value was a NaN, meaning Not a Number. What this means in practice, is that some of the reciept totals must be negative. This is quite unexpected, and warrants investigation. As for the code, there are some points to be made. Firstly, the SQL statement is sent to the server with the dbQuery function, which takes as arguments our Oracle connection and the SQL statement we want to excute. The output from this function (stored in the dfq variable) is simply some summary information about the query, not the actual result returned from the query. The result is returned from the next statement, the fetch(dfq) function. The data frame dfd2 is now a standard R dataframe containing the result of the query as we would expect. Note that all column names returned from Oracle are uppercase, regardless of how you capitalized them in the SQL statement. Secondly, the dataframe that is returned can be fed directly into the ggplot function from the excellent ggplot2 library. Books have been written about ggplot, and we are not going to go into the library in details. But the code basically does the following: 1. It passes the two variables we want in our plot into the function aes which stands for aesthetic. 2. It then adds an element to the visualization, geom_jitter, that, based on the two variables we specified in the line above, creates a scatterplot of country and sales. Because country is a category variable, we don't want all our points to fall on a single line. That is why we use the geom_jitter function instead of the geom_point function. We are adding some random offset for each point on the X-axis, resulting in wide strokes that gives a better visual indication of the distribution. 3. Lastly, because the values varies so widely on the Y-axis, we add a logarithmic Y-axis. By default a linear Y-axis would have been added (without us having to specify anything), but adding our own axis overwrites the default. ggplot2 is a great library for visualizations, and if you want to learn more, one starting point is the documentation (littered with examples) at ggplot2.tidyverse.org. Another thing that happened in the code above, was a subquery. Instead of specifying that the country name should be EIRE or France or Germany or United Kingdom, we use an in() function, which usually takes a comma-separated list of accepted values, and replace the list with an entire SQL query that returns the country names we want. We can even run the subquery by itself, to see that it indeed is entirely valid and returns the name of the 4 largest countries in our dataset. SELECT country FROM trx GROUP BY country ORDER BY COUNT(*) desc FETCH FIRST 4 ROWS ONLY Table 6.7: 4 records COUNTRY United Kingdom Germany France EIRE Combining queries like this can be powerful, and the query and subquery could be from different tables - as long as the countries are spelled the same, our database won't care. Other ways of combining data will be covered when we start grappling with some of the sample data that comes with Oracle. 6.2 More ways to JOIN tables Subqueries are great, but using joins to combine the content of multiple tables is just as frequent a task. In order to demonstrate a good use case for joins, we will take a quick detour from our transaction data and visit some of the sample data that came with the BigData Lite VM. Using the moviedemo user, you will find three tables named movie,movie_cast and cast. These tables are laid out in a way quite typical for relational databases. The movie table contains basic information about movies, such as their name, budget, and year produced as well as a movie ID to uniquely identify the movie. The cast table contains the name and a unique ID of every actor in these movies. Because one movie uses many actors, and one actor can star in several movies, the movie-cast table functions as a lookup table listing which actor played in which movie. Using joins, we can select movie titles (from the movie table) and cast names (from the cast table), using the movie_cast table: SELECT title, year, name FROM movie m INNER JOIN movie_cast mc ON m.movie_id=mc.movie_id INNER JOIN cast c ON c.cast_id=mc.cast_id WHERE year BETWEEN 1900 AND 1910 ORDER BY title Table 6.8: Displaying records 1 - 10 TITLE YEAR NAME A Corner in Wheat 1909 James Kirkwood A Corner in Wheat 1909 Linda Arvidson A Corner in Wheat 1909 Grace Henderson A Corner in Wheat 1909 H. B. Walthall A Corner in Wheat 1909 Frank Powell A Corner in Wheat 1909 W. C. Miller A Corner in Wheat 1909 Gladys Egan A Corner in Wheat 1909 Blanche Sweet Le voyage dans la lune 1902 Jeanne d'Alcy Le voyage dans la lune 1902 Georges Melies We are not displaying any information from the movie_cast table, insted we are simply using this table to join the movie table with the cast table. A word of caution about joins: If the columns you are joining on not uniquely identifies rows in two or more of the tables, you end up with what is known as many-to-many joins. SQL handles these situations by creating a carthesian join, meaning it creates all possible combinations of the rows. If you happen to have two rows with identical ID in both tables, the result will contain 2 x 2 = 4 rows with that ID. Although this behavior can be useful at times, it is normally not what you want to see happen. 6.3 A few common gotchas Before diving deep into data exploration, there are a couple of novelties in how SQL works that is worth mentioning. These are not unique to Oracle or SQL, but depending on the language you come from these behaviours might trip you. So take some time now to make sure you are aware of them. 6.3.1 Missing values You might be familiar with how R handles missing values, SQL has a similar but somewhat different approach, but both R and SQL are based on set theory. Missing values in SQL are referred to as NULLs. NULL values are different from all other values, and neither smaller, larger nor equal any other value. NULLs are not even equal NULL. If you want to select rows where a given variable is NULL, you have to use the special condition IS NULL. For example, we can count the number of customers we don't have an address of: SELECT COUNT(1) AS missing_addresses FROM customer WHERE street_address IS NULL Table 6.9: 1 records MISSING_ADDRESSES 2191 Out of our almost 5 000 customers, we don't have an address on almost 3 000. In aggregate functions though, NULLs are ignored by default and do not usually require special treatment. We can show this by selecting the number of street addresses in the customer table, which will ignore the missing addresses: SELECT COUNT(street_address) AS num_addr FROM customer Table 6.10: 1 records NUM_ADDR 2657 This will be identical to the number of rows where the address is not missing: SELECT COUNT(1) AS num_addr FROM customer WHERE street_address IS NOT NULL Table 6.11: 1 records NUM_ADDR 2657 The same behavior will be true for other aggregate functions, such as AVG(). 6.3.2 Execution order When you program in R or most other languages, you expect the machine to throw an error and exit after the first error in the code. After fixing this one, subsequent errors may manifest as the executor reaches them. SQL is not most other languages. The execution order of the script is not linear from top to bottom, and as a result the order in which errors are thrown might seem arbitrary. The appendix gives a little more detail on the execution order, but for now just know that errors are not necessarily given to you in a left-to-right fashion. 6.3.3 ANSI and the dialects For better and worse, SQL is not one language. Much of the basic syntax and behavior is specified by ANSI (American National Standards Institute), and is frequently referred to as ANSI SQL. But any given database vendor can choose to leave out certain features, and also to implement additional features not in ANSI. In the later years, Oracle has stived to implement all of ANSI. But Oracle also implements a host of other features that are not in ANSI. Some of these features are extremely powerful, and we will spend time discussing many of them. "],["intermediate-sql-intermediate-sql.html", "Chapter 7 Intermediate SQL {intermediate-sql} 7.1 Create conditional statements with CASE WHEN 7.2 Filter on aggregates with HAVING 7.3 More about limiting the result with FETCH FIRST 7.4 Simple convenience functions 7.5 What we are not going to cover", " Chapter 7 Intermediate SQL {intermediate-sql} You now have a good knowledge of basic SQL, and you are able to create subsets, aggregations and new combinations of data. This is already enough to make good use of any database you have access to, allowing you leverage the database's data crunching abilities to extract results much more tailored to your needs than simply copying the entire table from Oracle to R. 7.1 Create conditional statements with CASE WHEN SQL is not made for traditional programming with looping, conditional operations, and variable assignments i the fast and loose way that we might be used to from other languages. SQL is operations on datasets, but there is a syntax for if-else statements that comes in handy: CASE WHEN. We noted earlier that some of the sales in our transaction data seems to be negative - which is unexpected. In real world data, surprises like this are common, and often come about because you lack knowledge of where the data comes from. Indeed, if you have experience from sales, you might be bored right now thinking of course some reciepts are negative! Have you never heard of a return? Let's explore the negative values a little more. Since it was the product of unitPrice and Quantity that was negative, we are looking for rows where either one of them is negative (but strictly speaking it would be OK if both of them were). As so often, we start by just looking at a few examples to get a feel for what we are dealing with. SELECT * FROM trx WHERE unitprice&lt;0 OR quantity&lt;0 FETCH FIRST 10 ROWS ONLY Table 7.1: Displaying records 1 - 10 INVOICENO STOCKCODE DESCRIPTION QUANTITY INVOICEDATE UNITPRICE CUSTOMERID COUNTRY C536391 22556 PLASTERS IN TIN CIRCUS PARADE -12 2010-12-01 10:24:00 1.65 17548 United Kingdom C536391 21984 PACK OF 12 PINK PAISLEY TISSUES -24 2010-12-01 10:24:00 0.29 17548 United Kingdom C536391 21983 PACK OF 12 BLUE PAISLEY TISSUES -24 2010-12-01 10:24:00 0.29 17548 United Kingdom C536391 21980 PACK OF 12 RED RETROSPOT TISSUES -24 2010-12-01 10:24:00 0.29 17548 United Kingdom C536391 21484 CHICK GREY HOT WATER BOTTLE -12 2010-12-01 10:24:00 3.45 17548 United Kingdom C536391 22557 PLASTERS IN TIN VINTAGE PAISLEY -12 2010-12-01 10:24:00 1.65 17548 United Kingdom C536391 22553 PLASTERS IN TIN SKULLS -24 2010-12-01 10:24:00 1.65 17548 United Kingdom C536379 D Discount -1 2010-12-01 09:41:00 27.50 14527 United Kingdom C536383 35004C SET OF 3 COLOURED FLYING DUCKS -1 2010-12-01 09:49:00 4.65 15311 United Kingdom C536506 22960 JAM MAKING SET WITH JARS -6 2010-12-01 12:38:00 4.25 17897 United Kingdom Immediately, we notice that these are often quite normal-sounding products, and the quantity is negative, not the price. Let's check that theory. SELECT * FROM trx WHERE unitprice&lt;0 FETCH FIRST 10 ROWS ONLY Table 7.2: 2 records INVOICENO STOCKCODE DESCRIPTION QUANTITY INVOICEDATE UNITPRICE CUSTOMERID COUNTRY A563186 B Adjust bad debt 1 2011-08-12 14:51:00 -11062 NA United Kingdom A563187 B Adjust bad debt 1 2011-08-12 14:52:00 -11062 NA United Kingdom Out of over 500 000 rows, 2 have a negative unit price because they seem to be striking bad debt that they can't collect. Now, we can use the CASE WHEN clause to find the total effect of these negative values. SELECT SUM(CASE WHEN quantity&gt;0 AND unitPrice&gt;0 THEN quantity*unitPrice ELSE NULL END) AS positive_sales, SUM(quantity*unitPrice) AS all_sales FROM trx Table 7.3: 1 records POSITIVE_SALES ALL_SALES 10666685 9747748 Had the shop been able to stave off bad debt and the negative quantities that we for now presume to be returns, it would have 10 million pounds in revenue. The difference, close to 900 000 ponds, is about 10 % of sales. In order to find out if this is to be expected, we would need to talk to someone with knowledge of the business. From a technical standpoint, the SQL sums either the product of quantity and unitPrice if they are both positive, or NULL (which the sum function implicitly omits) if one or both of them is negative. This gives us the sum of positive sales right next to the sum of all sales. Thanks to CASE WHEN there was no need to split this into two queries. In R, aggregation functions usually return NA (the R version of NULL) by default if one or more values it is aggregating is NA. SQL simply omits missing values, pretending they aren't even there. Other Databases Oracle requires any group by variables to be included in the select statement too. Some other databases do not require this. Likewise, the group by statement can not include the shortnames assigned to the variables 7.2 Filter on aggregates with HAVING When we found the 4 biggest countries, we used the rownum keyword in combination with ORDER BY to get the 4 biggest countries. 4 was an arbitrary cutoff, maybe we are interested in all countries with more than 100 000 pounds in total sales? The WHERE clause only works on the underlying rows, not on computed aggregates. Instead, there is a seperate keyword, HAVING, that lets you filter on the aggregated values. Let's find the countries with more than 100 000 in revenue. SELECT country, SUM(quantity*unitPrice) AS sales FROM trx GROUP BY country HAVING SUM(quantity*unitPrice)&gt;100000 ORDER BY 2 desc There is a total of 6 contries that sold for mor than 100 000 pounds, with Australia closest to the cutoff with 137 000 in sales. Unfortunately, HAVING doesn't understand the column names we assign in SELECT (sales in this case), so we need to repeat the entire expression when filtering. ORDER BY, on the other hand, is perfectly happy sorting by either sales or simply the column number as shown here. 7.3 More about limiting the result with FETCH FIRST We have already seen how to return only a given number of rows with the FETCH FIRST command. Some may be familiar with another way of limiting results, using the ROWNUM pseudo-column like this: SELECT country, description FROM trx WHERE rownum &lt; 10 (#tab:rownum_sql)9 records COUNTRY DESCRIPTION United Kingdom CREAM HEART CARD HOLDER United Kingdom ENAMEL FLOWER JUG CREAM United Kingdom ENAMEL FIRE BUCKET CREAM United Kingdom ENAMEL BREAD BIN CREAM United Kingdom SET 3 WICKER OVAL BASKETS W LIDS United Kingdom JAM MAKING SET PRINTED United Kingdom JAM MAKING SET WITH JARS United Kingdom JUMBO BAG DOLLY GIRL DESIGN United Kingdom TRADITIONAL CHRISTMAS RIBBONS This works just fine in many situations, but has two drawbacks. For one, it doesn't work on aggregates. The following query would fail: SELECT country, COUNT(1) FROM trx GROUP BY country HAVING rownum &lt; 10 FETCH FIRST on the other hand, works like a charm. FETCH FIRST can also be combined with an offset, letting you fetch, say, row 11-20 instead of row 1-10. This is not possible with ROWNUM, as the first row returned is always row one. Including WHERE ROWNUM&gt;10 in a query will return an empty resultset. Offsets can be handy in, for example, the classical example of cycling through a paginated list of items. You have a large number of rows, ordered by some condition, and you want to show only 10 at a time, with a &quot;next page&quot; link at the bottom of the list. If we want to show rows 11-20, we can simply modify the query slightly. SELECT country, SUM(quantity*unitPrice) AS sales FROM trx GROUP BY country ORDER BY sales desc OFFSET 10 ROWS FETCH NEXT 10 ROWS ONLY (#tab:country_groupby_sum_mult)Displaying records 1 - 10 COUNTRY SALES Japan 35341 Norway 35163 Portugal 29367 Finland 22327 Channel Islands 20086 Denmark 18768 Italy 16891 Cyprus 12946 Austria 10154 Hong Kong 10117 7.4 Simple convenience functions 7.4.1 The decode function Sometimes, a CASE WHEN statement can be a little much. Enter the decode function, which is a type of case-when statement as a function. The decode function takes an even number of arguments. The first value is the variable or expression you want to check. The second and the third are the value you are checking against and the value you want returned, respectively. This argument pair can be repeated any number of times. Lastly, there is a single argument to specify the else value, which is returned if none of the previously stated happen. Say, you want to quickly recode the gender from 'Male' and 'Female' to the german equivalents. This can be done as follows: SELECT DECODE(gender, &#39;Male&#39;, &#39;Männlich&#39;, &#39;Weiblich&#39;) FROM customer (#tab:decode_first_example)Displaying records 1 - 10 DECODE(GENDER,'MALE','MANNLICH','WEIBLICH') Mannlich Mannlich Mannlich Mannlich Mannlich Mannlich Mannlich Weiblich Mannlich Mannlich If we want to take care of potential missing/bad values in the data, we have to add an unknown category. If we don't, all values not equal to 'Male' will return with the female label. At the same time, lets use what we have learned about aggregations and unions to check that the result is correct (and learn something about the gender balance among our customers). SELECT DECODE(gender, &#39;Male&#39;, &#39;Männlich&#39;, &#39;Female&#39;, &#39;Weiblich&#39;, &#39;Nicht spezifiziert&#39;) AS geschlecht, COUNT(1) AS anzahl FROM ( SELECT gender FROM customer UNION ALL SELECT NULL AS gender FROM dual ) GROUP BY DECODE(gender, &#39;Male&#39;, &#39;Männlich&#39;, &#39;Female&#39;, &#39;Weiblich&#39;, &#39;Nicht spezifiziert&#39;) ORDER BY 2 DESC (#tab:decode_function)3 records GESCHLECHT ANZAHL Mannlich 2479 Weiblich 2369 Nicht spezifiziert 1 The NULL value triggers the other part of the decode function, and results in one extra row and one entry in the &quot;unspecified&quot; category. 7.4.2 upper / lower In many search and language processing applications it is vital to omit differences in capitalization. The fastest way to do this is usually to convert everything to upper- or lowercase, which can be done easily in SQL with the UPPER() and LOWER() function. These functions are quite uncomplicated. The short example below converts email-addresses to both upper and lower case. SELECT email, UPPER(email), LOWER(email) FROM customer FETCH FIRST 5 ROWS ONLY (#tab:upper_lower)5 records EMAIL UPPER(EMAIL) LOWER(EMAIL) Laurel.Recker@oraclemail.com LAUREL.RECKER@ORACLEMAIL.COM laurel.recker@oraclemail.com Otto.Suzuki@oraclemail.com OTTO.SUZUKI@ORACLEMAIL.COM otto.suzuki@oraclemail.com Ta-Heng.Klossovsky@oraclemail.com TA-HENG.KLOSSOVSKY@ORACLEMAIL.COM ta-heng.klossovsky@oraclemail.com Bhadrabuja.Rovigo@oraclemail.com BHADRABUJA.ROVIGO@ORACLEMAIL.COM bhadrabuja.rovigo@oraclemail.com Candrin.Lyakhov@oraclemail.com CANDRIN.LYAKHOV@ORACLEMAIL.COM candrin.lyakhov@oraclemail.com 7.4.3 substring Text strings are often very well-structured, and you may know exactly which characters in the column you are interested in. The SUBSTR function takes two (optionally three) arguments. First, the column you want to find a substring in. Second, the letter number you want to start reading from, and optionally the number of letters you want to return. In the customer table, the INCOME_LEVEL column consists of a letter, followed by the actual income bracket. Suppose we want to return only the letter in our query, or only the income level, the SUBSTR function is fast and easy because we always know which position the letters and dollar amounts have. SELECT gender, SUBSTR(income_level, 1,1), SUBSTR(income_level, 4) FROM customer FETCH FIRST 5 ROWS ONLY (#tab:substr_first5)5 records GENDER SUBSTR(INCOME_LEVEL,1,1) SUBSTR(INCOME_LEVEL,4) Male E 90,000 - 109,999 Male E 90,000 - 109,999 Male A Below 30,000 Male C 50,000 - 69,999 Male F 110,000 - 129,999 7.4.4 Regular expressions Making sense of freeform text is always a challenge, and regular expressions (or regex in jargon) is a powerful tool for the task. Regex is a way too complex subject to explain in detail here, but we do afford ourselves a few examples from Oracle's regex functions. In short, regex allows you to find patterns in text, such as extracting only digits from a field, validating email addresses, or select all capitalized words that occur in groups of two or more (suspected personal names). SELECT COUNT(*) FROM customer WHERE REGEXP_LIKE(email, &#39;[a-zA-Z0-9_\\.]+@[a-zA-Z0-9_\\.]+\\.[a-zA-Z]&#39;) (#tab:regex_count)1 records COUNT(*) 4832 We could also extract last names (as in: last word in the name) from the cast table, and see what last names are most popular in Hollywood. SELECT regexp_substr(name, &#39;\\w+$&#39;) AS last_name, COUNT(1) AS occurences FROM crew GROUP BY regexp_substr(name, &#39;\\w+$&#39;) ORDER BY 2 DESC (#tab:regex_groupby)Displaying records 1 - 10 LAST_NAME OCCURENCES NA 84 small 60 Smith 31 Johnson 23 Jones 22 Miller 20 Williams 17 Cohen 16 Wilson 16 Lee 16 There are other regex functions in Oracle, that lets you replace text (REGEXP_REPLACE) and count the number of occurences of a pattern in a string (REGEX_COUNT). 7.4.5 Dealing with dates There is a good chance that a table near you has a column that represents time in some sense. There is also a good chance that the column is stored as something other than a date. Sometimes this is reasonable, but a lot of the time, storing dates and times as actual dates and times can be very beneficial. We already touched on the concept of dates in chapter 5, but we did not elaborate on it. What we did, was to instruct the database that the column INVOICEDATE should be interpreted as a date, and specify the date format in a separate argument. In our case, we were dealing with not just a date but with a timestamp containing the date in the format YYYY-MM-DD followed by a space followed by the time in the format of HH24:MI:SS - that is the hour (zulu-time, 24 hours), colon, minutes (00-59), colon, seconds (00-59). The entire line of code called the TO_DATE function, with the argument is TO_DATE(INVOICEDATE, 'YYYY-MM-DD HH24:MI:SS') AS INVOICEDATE. Having a column properly defined as a date, you can start doing some neat stuff like subtracting two dates from each other, which returns the number of days in between. Try it yourself: SELECT TO_DATE(&#39;01-01-2017&#39;, &#39;DD-MM-YYYY&#39;) - TO_DATE(&#39;01-01-2016&#39;, &#39;DD-MM-YYYY&#39;) AS time_diff FROM DUAL (#tab:sql_date_diff)1 records TIME_DIFF 366 If you think the database must be of its rockers to suggest there was 366 days between january 1st 2016 and january 1 2017, remember that 2016 was a leap year. R has the same concepts, a similar way of parsing dates (which you also saw briefly in chapter 5), and is also in agreement that there were 366 days in between january 1st 2016 and january 1st 2017. strptime(&quot;01/01/2017&quot;, format = &quot;%m/%d/%Y&quot;) - strptime(&quot;01/01/2016&quot;, format = &quot;%m/%d/%Y&quot;) ## Time difference of 366 days Once your column is defined as a date, there are some other nice features built in that can be very useful. Dates can be converted to many other formats, such as week number, or day of week. Lets check our sales by day of week: SELECT TO_CHAR(invoicedate, &#39;DAY&#39;), SUM(quantity) AS units_sold FROM trx GROUP BY TO_CHAR(invoicedate, &#39;DAY&#39;) ORDER BY 2 DESC (#tab:convert_to_day)6 records TO_CHAR(INVOICEDATE,'DAY') UNITS_SOLD THURSDAY 1167823 WEDNESDAY 969558 TUESDAY 961543 MONDAY 815354 FRIDAY 794440 SUNDAY 467732 There is definetely something odd about this data - there were no sales on saturdays, but there were sales on sunday. Lets look at sales by week as well. But we don't want a 52-row output table to browse, so we'll do the aggregation in SQL and pass the result to R and ggplot. q &lt;- dbSendQuery(con, &quot; SELECT TO_CHAR(invoicedate, &#39;WW&#39;) AS WEEK, SUM(quantity) AS UNITS_SOLD FROM trx GROUP BY TO_CHAR(invoicedate, &#39;WW&#39;) ORDER BY 2 DESC &quot;) df &lt;- fetch(q) library(ggplot2) library(dplyr) df %&gt;% ggplot(aes(WEEK, UNITS_SOLD)) + geom_bar(stat = &#39;identity&#39;) Ggplot needs to know that this data is already aggregated, and we specify this by the argument stat='identity' that we pass to geom_bar. Hopefully needless to say, R has similar capabilities in expressing dates as weeks and aggregating. As an example, lets try to make the same chart using R. q &lt;- dbSendQuery(con, &quot; SELECT invoicedate, quantity FROM trx &quot;) df &lt;- fetch(q) %&gt;% mutate(week = strftime(INVOICEDATE, format = &quot;%V&quot;)) library(ggplot2) library(dplyr) df %&gt;% ggplot(aes(week)) + geom_bar(aes(weight = QUANTITY) ) As we hoped for, the charts look identical, and its up to you to decide which one to use. If the data is big enough, you probably want to do leave the aggregation to the database. 7.5 What we are not going to cover 7.5.1 PL/SQL PL/SQL is Oracle's programming language extension to SQL. It lets you declare variables, run conditional statements and loops, in order to generate SQL statements on the fly. Since we are using R which usually covers these use cases, there are few arguments for learning PL/SQL too. 7.5.2 the dbplyr library If you prefer avoid SQL when possible, you should consider using a library called dbplyr. This is an interface that lets you write code that should be familiar to any dplyr user, that is then run in the database as SQL. The authors have done a great job at optimizing the code, so that the resulting database queries are every bit as performant as you could hope for. But, like many libraries, it has its limitations as well. To illustrate examples of when this works and when it doesn't, we will try to get a summary of sales by country like we did in chapter 6.1.2, and generate summaries by weekday like we did above. library(dbplyr) #define a r-object that references the database table trxtbl &lt;- tbl(con, &quot;TRX&quot;) # Run dplyr-code just like it was a regular data frame trxtbl %&gt;% group_by(COUNTRY) %&gt;% summarize(total_sales = sum(QUANTITY*UNITPRICE)) %&gt;% collect() %&gt;% arrange(desc(total_sales)) %&gt;% top_n(3) ## Selecting by total_sales ## # A tibble: 3 x 2 ## COUNTRY total_sales ## &lt;chr&gt; &lt;dbl&gt; ## 1 United Kingdom 8187806. ## 2 Netherlands 284662. ## 3 EIRE 263277. But if we try to do the same with weekdays, we get a strange error-message. trxtbl %&gt;% mutate(week = strftime(INVOICEDATE, format=&quot;%V&quot;)) group_by(week) %&gt;% summarize(total_sales = sum(QUANTITY*UNITPRICE)) %&gt;% arrange(desc(total_sales)) %&gt;% top_n(3) Error in .oci.SendQuery(conn, statement, data = data, prefetch = prefetch, : ORA-00907: missing right parenthesis It is easy to see what went wrong, by passing the result to the show_query() function. We can do this in the same workflow, by just piping to show_query(). trxtbl %&gt;% mutate(week = strftime(INVOICEDATE, format=&quot;%V&quot;)) %&gt;% group_by(week) %&gt;% summarize(total_sales = sum(QUANTITY*UNITPRICE)) %&gt;% arrange(desc(total_sales)) %&gt;% top_n(3) %&gt;% show_query() SELECT &quot;week&quot;, &quot;total_sales&quot; FROM (SELECT &quot;week&quot;, &quot;total_sales&quot;, rank() OVER (ORDER BY &quot;total_sales&quot; DESC) AS &quot;zzz12&quot; FROM (SELECT * FROM (SELECT &quot;week&quot;, SUM(&quot;QUANTITY&quot; * &quot;UNITPRICE&quot;) AS &quot;total_sales&quot; FROM (SELECT &quot;INVOICENO&quot;, &quot;STOCKCODE&quot;, &quot;DESCRIPTION&quot;, &quot;QUANTITY&quot;, &quot;INVOICEDATE&quot;, &quot;UNITPRICE&quot;, &quot;CUSTOMERID&quot;, &quot;COUNTRY&quot;, STRFTIME(&quot;INVOICEDATE&quot;, '%V' AS &quot;format&quot;) AS&quot;week&quot; FROM (&quot;TRX&quot;) ) &quot;egcdmyvzjl&quot; GROUP BY &quot;week&quot;) &quot;jtxdupatwa&quot; ORDER BY &quot;total_sales&quot; DESC) &quot;tguwvduhof&quot;) &quot;fgqwxodxti&quot; WHERE (&quot;zzz12&quot; &lt;= 3.0) dbplyr has generated quite a large query for us, but it passed the r-specific strftime function into the SQL statement, which is what caused the error. SQL and R-code don't mix. The generated code ran at a cost of 3147 once the strftime function had been replaced with TO_CHAR(). Writing the same statement myself somewhat differently resulted in a cost of 3020 - a neglible difference. It is also interesting to see how dbplyr utilizes windowing functions, which is a fairly new addition to the SQL language. We will learn more about windowing functions in 9.1 "],["aggregation-functions-aggregation-functions.html", "Chapter 8 Aggregation functions {aggregation-functions} 8.1 Median and quantiles 8.2 First values {first-values} 8.3 Aggregating text", " Chapter 8 Aggregation functions {aggregation-functions} Since we have already dived deep into aggregations in ??, this chapter will expand on aggregations by going through some of the other aggregation functions found in Oracle. 8.1 Median and quantiles Medians are easy to compute, simply switch AVG with MEDIAN and you are set. Quartiles, deciles and percentiles give you a lot of options, and so they are not implemented in the same way - hence, the syntax is a bit trickier. Let's look at an example, again from the transactions table, where we compare the median, quartiles, P10 and P90 of the unitprice between countries. SELECT country, PERCENTILE_DISC(0.1) WITHIN GROUP(ORDER BY unitprice) AS p10_price, PERCENTILE_DISC(0.25) WITHIN GROUP(ORDER BY unitprice) AS q1_price, MEDIAN(unitprice) AS median_price, PERCENTILE_DISC(0.5) WITHIN GROUP(ORDER BY unitprice) AS p50_price, PERCENTILE_DISC(0.75) WITHIN GROUP(ORDER BY unitprice) AS q3_price, PERCENTILE_DISC(0.9) WITHIN GROUP(ORDER BY unitprice) AS p90_price FROM trx GROUP BY country ORDER BY COUNT(1) DESC FETCH FIRST 10 ROWS ONLY (#tab:pctl_sql)Displaying records 1 - 10 COUNTRY P10_PRICE Q1_PRICE MEDIAN_PRICE P50_PRICE Q3_PRICE P90_PRICE United Kingdom 0.65 1.25 2.1 2.1 4.1 8.0 Germany 0.55 1.25 1.9 1.9 3.8 8.2 France 0.55 1.25 1.8 1.8 3.8 8.0 EIRE 0.55 1.25 2.1 2.1 4.2 8.5 Spain 0.65 1.25 2.1 2.1 4.2 8.5 Netherlands 0.42 0.85 1.4 1.4 2.5 5.0 Belgium 0.55 1.25 1.9 1.9 4.2 9.9 Switzerland 0.42 1.25 1.7 1.7 3.8 8.0 Portugal 0.65 1.25 1.6 1.6 3.2 6.0 Australia 0.55 1.25 1.8 1.8 3.8 6.0 To abbreviate the output, we have ordered the output by number of sales, and limited the result to the ten countries with the most sales. The MEDIAN function is simple, taking only the unitprice as input. As you can see from the median_price and p50_price columns, this is equivalent to taking the 50th percentile. The PERCENTILE_DISC function which returns the percentiles, is somewhat more involved. In order to understand the logic, remember what percentiles (and quartiles and medians) really are: They are the value at a given observation, after the data has been sorted in ascending order by that value. Therefore, the PERCENTILE_DISC function only takes the percentile as an argument. The variable we are interested in, unitprice, is specified in the WITHIN GROUP statement, as the variable we are ordering by. Because the SQL syntax is quite consistent, you could even add some confusion to your calculations by altering the sort order. Try it yourself by writing ORDER BY unitprice DESC in the WITHIN GROUP statement for p10_price. Suddenly you will have the 90th percentile instead of the 10th. A second, smaller cause for confusion is the function name, PERCENTILE_DISC. For our purposes, this is the normal percentile function we want to use. But there is a different percentile function, PERCENTILE_CONT, which tries to linearly interpolate the values and return the given percentile from that interpolated function. 8.2 First values {first-values} Another interesting thing to grab, is the first or last value of some variable, ordered by some other variable. To illustrate this, ask yourself: Are customers likely to buy larger quantities of products with a low unit price, or a high unit price? If we want to investigate this, we can select the minimum quantity purchased of the cheapest product sold in each country, and the maximum quantity sold of the most expensive product sold in each country. SELECT country, AVG(quantity) KEEP (DENSE_RANK FIRST ORDER BY unitprice) AS min_q_cheap_products, AVG(quantity) KEEP (DENSE_RANK LAST ORDER BY unitprice) AS max_q_expensive_products FROM trx GROUP BY country ORDER BY COUNT(1) DESC FETCH FIRST 10 ROWS ONLY Table 8.1: Displaying records 1 - 10 COUNTRY MIN_Q_CHEAP_PRODUCTS MAX_Q_EXPENSIVE_PRODUCTS United Kingdom 1 -1.0 Germany 1 0.0 France 1 0.0 EIRE 68 0.0 Spain 11 -1.0 Netherlands 144 -1.0 Belgium 39 1.0 Switzerland 1 3.1 Portugal 20 0.0 Australia 103 1.0 8.3 Aggregating text Aggregating numbers is one thing, but averages and sums don't lend themselves as nicely to text data. What Oracle can do however, is to concatenate text for you. Say, for example, you want to collect all the products of an order in a single cell. This can be done with the special LISTAGG() function, which takes two arguments: The text field, and whatever character string you want to separate the concatenated cells by. In addition (because this just couldn't be this simple), the function must be followed by a WITHIN GROUP statement, in which you specify how the cells should be ordered before concatenation. SELECT INVOICENO, COUNT(1) AS number_of_goods, LISTAGG(DESCRIPTION, &#39;, &#39;) WITHIN GROUP ( ORDER BY stockcode) AS goods FROM TRX --WHERE INVOICENO IN(&#39;536384&#39;, &#39;536385&#39;) GROUP BY INVOICENO FETCH FIRST 3 ROWS ONLY Table 8.2: 3 records INVOICENO NUMBER_OF_GOODS GOODS 536365 7 GLASS STAR FROSTED T-LIGHT HOLDER, SET 7 BABUSHKA NESTING BOXES, WHITE METAL LANTERN, RED WOOLLY HOTTIE WHITE HEART., KNITTED UNION FLAG HOT WATER BOTTLE, CREAM CUPID HEARTS COAT HANGER, WHITE HANGING HEART T-LIGHT HOLDER 536366 2 HAND WARMER RED POLKA DOT, HAND WARMER UNION JACK 536367 12 HOME BUILDING BLOCK WORD, LOVE BUILDING BLOCK WORD, RECIPE BOX WITH METAL HEART, IVORY KNITTED MUG COSY , BOX OF VINTAGE ALPHABET BLOCKS, BOX OF VINTAGE JIGSAW BLOCKS , POPPY'S PLAYHOUSE BEDROOM , POPPY'S PLAYHOUSE KITCHEN, FELTCRAFT PRINCESS CHARLOTTE DOLL, DOORMAT NEW ENGLAND, ASSORTED COLOUR BIRD ORNAMENT, BOX OF 6 ASSORTED COLOUR TEASPOONS In this query, we select three columns, two of which are calculated in the aggregation. The COUNT is simple enough, the LISTAGG deserves some further explanation. The item description is in the aptly named DESCRIPTION field, which is the first argument to LISTAGG. The second argument is a comma followed by space, in single quotes, specifying that we want the descriptions separated by a space and a comma - which makes the text readable. The WITHIN GROUP clause is not optional even if you couldn't care less about the order in which the descriptions appear. You can, of course, arrange the items alphabetically by ordering by the description. "],["analytical-functions.html", "Chapter 9 Analytical functions 9.1 Over the partitions and far away 9.2 Models", " Chapter 9 Analytical functions 9.1 Over the partitions and far away Even without aggregating, it is possible to create an average of a column. A new, computed column can be added to an existing selection by using the analytical average function. This is in fact the same function as the aggregate function we covered in last chapter, but with an added keyword to distinguish it from the aggregate function and add a host of new features. Let's start with the simplest example possible, taking the average of stock prices. SELECT symbol, tstamp, price, AVG(price) OVER() FROM ticker WHERE symbol=&#39;ACME&#39; ORDER BY tstamp Table 9.1: Displaying records 1 - 10 SYMBOL TSTAMP PRICE AVG(PRICE)OVER() ACME 2011-04-01 12 19 ACME 2011-04-02 17 19 ACME 2011-04-03 19 19 ACME 2011-04-04 21 19 ACME 2011-04-05 25 19 ACME 2011-04-06 12 19 ACME 2011-04-07 15 19 ACME 2011-04-08 20 19 ACME 2011-04-09 24 19 ACME 2011-04-10 25 19 The average stock price for the ACME corporation between April 1st and April 20th is added as a new column, and repeated on all rows. The OVER() statement is new, and used exclusively on these types of analytical functions. An empty OVER() statement means that we are taking the average over all the rows. But what if we don't want to restrict ourselves to ACME? What if we want to do this for all stocks in the table? We make use of the previously empty OVER() statement. Inside the paranthesis, you can add a PARTITION statement that takes the average over distinct values of some other column - just like the aggregate functions do with GROUP BY. Let's take a look, but this time we limit the query a little, to avid getting the whole 20 days for each stock. SELECT symbol, tstamp, price, AVG(price) OVER(PARTITION BY symbol) FROM ticker WHERE tstamp&lt;=TO_DATE(&#39;05042011&#39;, &#39;DDMMYYYY&#39;) ORDER BY symbol, tstamp Table 9.2: Displaying records 1 - 10 SYMBOL TSTAMP PRICE AVG(PRICE)OVER(PARTITIONBYSYMBOL) ACME 2011-04-01 12 19 ACME 2011-04-02 17 19 ACME 2011-04-03 19 19 ACME 2011-04-04 21 19 ACME 2011-04-05 25 19 GLOBEX 2011-04-01 3 6 GLOBEX 2011-04-02 7 6 GLOBEX 2011-04-03 8 6 GLOBEX 2011-04-04 4 6 GLOBEX 2011-04-05 8 6 As the name suggests, partitioning the data by symbol lets us create separate average for each stock. Better yet, we can compute a moving average of the price by adding just a few more statements. SELECT symbol, tstamp, price, AVG(price) OVER(PARTITION BY symbol ORDER BY tstamp ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW ) AS moving_avg FROM ticker WHERE tstamp&lt;=TO_DATE(&#39;05042011&#39;, &#39;DDMMYYYY&#39;) ORDER BY symbol, tstamp Table 9.3: Displaying records 1 - 10 SYMBOL TSTAMP PRICE MOVING_AVG ACME 2011-04-01 12 12.0 ACME 2011-04-02 17 14.5 ACME 2011-04-03 19 16.0 ACME 2011-04-04 21 17.2 ACME 2011-04-05 25 18.8 GLOBEX 2011-04-01 3 3.0 GLOBEX 2011-04-02 7 5.0 GLOBEX 2011-04-03 8 6.0 GLOBEX 2011-04-04 4 5.5 GLOBEX 2011-04-05 8 6.0 Now we're talking. A moving (cumulative) average for each stock. These averages are a great way to smooth out curves that otherwise would be too noisy to make sense of. If an average of all preceding rows are a bit too much, limit the preceding rows in the same way you limit following rows. Here are two more examples, one limiting to 5 rows preceding, and another taking the average from two preceding and two following. SELECT symbol, tstamp, price, AVG(price) OVER(PARTITION BY symbol ORDER BY tstamp ROWS BETWEEN 5 PRECEDING AND CURRENT ROW ) AS moving_avg_1, AVG(price) OVER(PARTITION BY symbol ORDER BY tstamp ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING ) AS moving_avg_2 FROM ticker WHERE tstamp&lt;=TO_DATE(&#39;05042011&#39;, &#39;DDMMYYYY&#39;) ORDER BY symbol, tstamp Table 9.4: Displaying records 1 - 10 SYMBOL TSTAMP PRICE MOVING_AVG_1 MOVING_AVG_2 ACME 2011-04-01 12 12.0 16.0 ACME 2011-04-02 17 14.5 17.2 ACME 2011-04-03 19 16.0 18.8 ACME 2011-04-04 21 17.2 20.5 ACME 2011-04-05 25 18.8 21.7 GLOBEX 2011-04-01 3 3.0 6.0 GLOBEX 2011-04-02 7 5.0 5.5 GLOBEX 2011-04-03 8 6.0 6.0 GLOBEX 2011-04-04 4 5.5 6.8 GLOBEX 2011-04-05 8 6.0 6.7 Finally, you don't have to specify the number of rows. Instead, you can specify the time interval given that you order by some variable of the date-variety. Let's make a 3-day moving average. Even though this is no different from specifying 3 rows in this specific dataset, it comes in handy when you are dealing with missing values, weekends, and arbitrary number of rows per day. SELECT symbol, tstamp, price, AVG(price) OVER(PARTITION BY symbol ORDER BY tstamp RANGE BETWEEN INTERVAL &#39;3&#39; DAY PRECEDING AND CURRENT ROW ) AS moving_avg_1 FROM ticker WHERE tstamp&lt;=TO_DATE(&#39;05042011&#39;, &#39;DDMMYYYY&#39;) ORDER BY symbol, tstamp Table 9.5: Displaying records 1 - 10 SYMBOL TSTAMP PRICE MOVING_AVG_1 ACME 2011-04-01 12 12.0 ACME 2011-04-02 17 14.5 ACME 2011-04-03 19 16.0 ACME 2011-04-04 21 17.2 ACME 2011-04-05 25 20.5 GLOBEX 2011-04-01 3 3.0 GLOBEX 2011-04-02 7 5.0 GLOBEX 2011-04-03 8 6.0 GLOBEX 2011-04-04 4 5.5 GLOBEX 2011-04-05 8 6.8 Instead of days, keywords like 'MONTH' and 'YEAR' can also be used. Many of the usual aggregate functions can be used as analutical functions, but notably median and quantiles are missing. These must be computed in R instead. 9.2 Models "],["statistical-functions.html", "Chapter 10 Statistical functions 10.1 Covariance: Revisitng quantity vs unitprice 10.2 Regression 10.3 T-tests 10.4 Kolmogorov-Smirnoff tests 10.5 And more...", " Chapter 10 Statistical functions Usually, statistics is best suited to do in R. But if your data is large enough and your needs aren't too advanced, Oracle packs an impressive array of statistical functions and can often execute them blazingly fast. 10.1 Covariance: Revisitng quantity vs unitprice In ??, we looked at the average quantity sold of the cheapest products, compared to the average quantity sold of the most expensive product in each country - the theory being that cheap products sell in higher quantities. Not the most provocative theory, but good enough for a teaching exercise. This question is probably better answered with some more advanced (and standard) statistical functions. We can start by taking the simple covariance between unitprice and quantity. SELECT country, COVAR_POP(unitprice, quantity) as price_quantity_covar FROM trx GROUP BY country ORDER BY COUNT(1) DESC FETCH FIRST 10 ROWS ONLY Table 10.1: Displaying records 1 - 10 COUNTRY PRICE_QUANTITY_COVAR United Kingdom -22 Germany -26 France -42 EIRE -71 Spain -31 Netherlands -111 Belgium -21 Switzerland -23 Portugal -72 Australia -105 To no surprise, we find a negative correlation between unit price and quantity sold in every country we cared to list. There are two covariance functions: COVAR_POP (which we used here), and COVAR_SAMP, which are asymptotically equal with high sample size. Covariance in small samples need to adjust for the one degree of freedom used, by removing 1 from \\(n\\): \\[\\frac{\\sum(expr1 * expr2) - \\sum expr1 * \\sum expr2}{n * (n-1)}\\] For COVAR_POP, the denominator is \\(n^2\\) instead of \\(n*(n-1)\\). 10.2 Regression Hopefully, the fact that Oracle databases does regression is impressive enough in itself, because the functionality is really limited to superficially simple cases. But if you need to do a regression with a single independent variable, Oracle is there for you. But seriously, do this in R instead if possible. SELECT REGR_INTERCEPT(quantity, unitprice) AS beta_0, REGR_SLOPE(quantity, unitprice) AS beta_1 FROM trx Table 10.2: 1 records BETA_0 BETA_1 9.6 0 The regression consists of running two different functions, one to calculate the intercept, and one to calculate the regression coefficient (and since we are limited to a single independent variable, there is only one slope to calculate). Still this syntax requires us to specify the model twice, and happily spits out a result even if we are unfortunate enough to, say, switch the arguments in one of the functions. The only redeeming feature of regression in Oracle is that it plays nicely with the GROUP BY statement, so that we with hardly any effort can create separate regression models for each country. SELECT country, COUNT(1) AS goods_sold, REGR_INTERCEPT(quantity, unitprice) AS beta_0, REGR_SLOPE(quantity, unitprice) AS beta_1 FROM trx GROUP BY country Table 10.3: Displaying records 1 - 10 COUNTRY GOODS_SOLD BETA_0 BETA_1 Poland 341 13.5 -0.68 Denmark 389 25.3 -1.30 Brazil 32 19.1 -1.78 United Kingdom 495478 8.6 0.00 Channel Islands 758 13.2 -0.15 United Arab Emirates 68 16.9 -0.74 Norway 1086 18.2 -0.08 Iceland 182 19.8 -2.38 Austria 401 14.1 -0.48 Saudi Arabia 10 14.4 -2.88 10.3 T-tests A lot of statistical tests assume that the data is normally distributed, which is far from the case in most real-world data. This example is meant to demonstrate an SQL funtion, and is unfortunately not sound statistical advice. Let's take a closer look at Germany and France in the sales data. These two countries have a similar number of sales (7-8000 each), and we might want to explore wether the customers buy similarly-priced items. First, a simple summary table to give a clue about the sales figures for the countries: SELECT COUNTRY, COUNT(1), median(unitprice), avg(unitprice), STDDEV(unitprice), MAX(unitprice) FROM trx WHERE country IN(&#39;Germany&#39;,&#39;France&#39;, &#39;EIRE&#39;) GROUP BY country Table 10.4: 3 records COUNTRY COUNT(1) MEDIAN(UNITPRICE) AVG(UNITPRICE) STDDEV(UNITPRICE) MAX(UNITPRICE) EIRE 8196 2.1 5.9 54 1917 France 8557 1.8 5.0 80 4161 Germany 9495 1.9 4.0 17 600 Now, for the t-test comparing France and Germany. Note that only two countries can be compared at the same time. SELECT STATS_T_TEST_INDEP(COUNTRY, unitprice, &#39;TWO_SIDED_SIG&#39; ) AS two_sided_p_value FROM trx WHERE country IN(&#39;Germany&#39;,&#39;France&#39;) Table 10.5: 1 records TWO_SIDED_P_VALUE 0.21 The two-sided T-test for independence cannot lead us reject our null-hypothesis that the average unitprice for products sold in Germany is the same as the average unitprice for products sold in France. Looking at the summary statistics this conclusion is a little strange, given that there is a large number of observations and the average unitprice in Germany and France are 4 and 5 (Euros?) respectively. But the standard deviation for france is very large - 80. Just for fun, let's take a look at the difference between Germany and Ireland as well. SELECT STATS_T_TEST_INDEP(COUNTRY, unitprice, &#39;TWO_SIDED_SIG&#39; ) AS two_sided_p_value FROM trx WHERE country IN(&#39;Germany&#39;,&#39;EIRE&#39;) Table 10.6: 1 records TWO_SIDED_P_VALUE 0 This time, we can safely reject the null-hypothesis that the average unit prices are equal. 10.4 Kolmogorov-Smirnoff tests As we noted at the top of the previous chapter, the t-test is really not the right test for this type of data, because the distribution is anything but normal. In these cases, we either have to find what distribution fits the data, or we can opt for a test that does not require a given distribution. Kolmogorov-Smirnoff (KS for short) is that type of test. The syntax for KS tests are beautifully similar to the t-test. We also print the KS-statistic. The third argument defaults to 'SIG' (significance), but we included it for clarity. SELECT STATS_KS_TEST(COUNTRY, unitprice, &#39;STATISTIC&#39;) AS KS_value, STATS_KS_TEST(COUNTRY, unitprice, &#39;SIG&#39;) AS significance FROM trx WHERE country IN(&#39;Germany&#39;,&#39;France&#39;) Table 10.7: 1 records KS_VALUE SIGNIFICANCE 0.03 0 The KS-test does not test for equality of the mean, but wether the two distributions a whole are sufficiently similar to believe that they were drawn from the same population. Intuitively, the function looks at the area with the biggest difference between the two samples and calculates wether that could be due to random choice. As we see, the KS-test leaves little doubt that there is a difference in the price of the goods bought in France and Germany, while we could not make that conclusion using the T-statistic. The difference is probably due to the long tail of high-price items. France had some much more expensive sales than Germany, which the T-test couldn't properly account for. 10.5 And more... These are two quite random examples of statistical tests in Oracle, but the list includes F-test (STATS_F_TEST), Mann-Whitney tests (STATS_MW_TEST), Wilcox (STATS_WSR_TEST) and ANOVA (STATS_ONE_WAY_ANOVA). "],["row-pattern-matching.html", "Chapter 11 Row pattern matching 11.1 A simple(ish) example 11.2 A more involved example", " Chapter 11 Row pattern matching So far, we have mostly been doing things that are equally possible to do in R, and hence the reason for doing it in SQL instead of R is probably performance-related. Row pattern matching is something quite different, to which there currently isn't any comparable functionality in any R libraries. That is bound to change sooner or later, but for the time being, Oracle has a big selling point in being able to do a type of processing that is both novel, useful and difficult to implement from scratch. Row pattern matching is, basically, a WHERE-statement on steroids. Instead of selecting individual rows, you can select whole sets of rows, if they, as a group satisfy the criteria you have written. The canonical example is stocks, where you may not be interested in selecting the days where the stock is above, say, 20, but you really want to find periods where the price was increasing. With row pattern matching you can do that - define what an increase means, and then say you want all rows that are part of, f.ex., periods of five or more consecutive days of price increases. In addition to the amped-up WHERE-clause, pattern matching lets you calculate new columns, called MEASURES, from the patterns that you define. Things like the difference between the start- and end-price in the aforementioned 5-or-more days of stock price increases. Also, the pattern matching can do aggregations, leaving you with only one row summing up each pattern found. 11.1 A simple(ish) example Although the stocks make a good and relatable intro to what pattern-matching can do, it is probably more powerful in other areas, with more feature-rich data. Nevertheless, we will start by doing a simple stock-price example where we select periods of stock price increases, however long or short. SELECT * FROM ticker MATCH_RECOGNIZE( PARTITION BY symbol ORDER BY tstamp ALL ROWS PER MATCH PATTERN ( up+ ) DEFINE up AS PREV(price)&lt;price ) Table 11.1: Displaying records 1 - 10 SYMBOL TSTAMP PRICE ACME 2011-04-02 17 ACME 2011-04-03 19 ACME 2011-04-04 21 ACME 2011-04-05 25 ACME 2011-04-07 15 ACME 2011-04-08 20 ACME 2011-04-09 24 ACME 2011-04-10 25 ACME 2011-04-13 25 ACME 2011-04-17 14 Although the query was short, there is a lot of new syntax here, starting with the MATCH_RECOGNIZE command that encompases the whole circus. Inside the match_recognize command, we start off by partitioning by symbol, which just like with the OVER clause says that we want to analyze patterns in different stocks in isolation. Secondly, we order by TSTAMP, which is the time column and denotes in which order things happened (the default ordering, ascending) usually agrees with how we want to analyze pattern in time. Now, for the really new stuff, which is probably easier to understand if we start with the last line. DEFINE is where we define the basic building blocks of what we want to match. We say that a row can be classified as &quot;UP&quot; if the stock price is higher that day than it was on the previous day. This is where the ORDER BY statement becomes important - without it, the previous row might not be the previous day, and our results wouldn't make much sense. In the PATTERN statement, we use the definition(s) we create in the DEFINE statement to specify what combinations of rows we are looking for. This step uses a lightweight version of regex to let us specify the patters, and the +-sign means the same here as in regex: One or more occurance. Finally, ALL ROWS PER MATCH is an explicit way of telling Oracle not to aggregate the results. In this query, we use match_recognize simply as a filter. To summarize, we use the DEFINE statement to definitions/labels/classifications of rows, with the help of functions such as PREV (previous row), NEXT, and some other methods that we will return to. We use these labels in the PATTERN statement, where we specify which combinations of these rows we are looking for, and finally we specify that we don't want to aggregate the results. The output of this, however, isn't easy to make sense of. The first four rows shows that the price increases from 17 to 25, but if we read the code carefully we see that 17 must already be an increase from the day before - otherwise it wouldn't have satisfied the UP definition. But we don't see what it increased from. Then, it drops from 25 to 15, and skips a day. Presumably, it found the pattern multiple times in the same stock, right next to each other. But we have a hard time reading this from the output, so let's add some more to the query. Firstly, we want to add the day the pattern starts. To do this we must add a definition to the pattern, but since we don't need this first row to satisfy any conditions other than coming before the first UP, we don't actually have to define it. If that sounds strange, you'll understand when you see it. Secondly, we want to see when one pattern ends and another starts. We can do this by adding a computed variable, called a MEASURE and a function called MATCH_NUMBER() that specifically enumerates what occurance of the pattern the row is part of. Additionally, since we now will have two different definitions as part of our pattern, we want to see what each row is classified as. This too we can do in the MEASURE clause, with the special function CLASSIFIER() which returns the name of the definition that triggered. SELECT * FROM ticker MATCH_RECOGNIZE( PARTITION BY symbol ORDER BY tstamp MEASURES MATCH_NUMBER() AS match_num, CLASSIFIER() as clf ALL ROWS PER MATCH PATTERN ( strt up+ ) DEFINE up AS PREV(price)&lt;price ) Table 11.2: Displaying records 1 - 10 SYMBOL TSTAMP MATCH_NUM CLF PRICE ACME 2011-04-01 1 STRT 12 ACME 2011-04-02 1 UP 17 ACME 2011-04-03 1 UP 19 ACME 2011-04-04 1 UP 21 ACME 2011-04-05 1 UP 25 ACME 2011-04-06 2 STRT 12 ACME 2011-04-07 2 UP 15 ACME 2011-04-08 2 UP 20 ACME 2011-04-09 2 UP 24 ACME 2011-04-10 2 UP 25 This helps immensly with readability. We can see that the start (nicknamed strt in the code) of the pattern is the 31st of march 2011, when the price is 12, before it increases to 17 which was the first observation we saw in the previous query. Also, we get a confirmation that the drop we saw when the price was 25, was the end of the first occurance of the pattern and the start of the second occurance. When we see a new ticker symbol, the match number counter resets. 11.2 A more involved example In order to really show the power of pattern matching, we have to import some more data. The New York City public payroll data is both public, rich, and has a panel-data structure that allows us to study changes over time. As of this writing, you can find the data at https://data.cityofnewyork.us/City-Government/Citywide-Payroll-Data-Fiscal-Year-/k397-673e. 11.2.1 Importing and cleaning You can download the dataset as CSV, and use the SQL Developer import wizard to create a table for it. If this was a dataset we were expecting to add more data to regularly, we would have set up a proper import script, probably in R, which would make the process automatic and reproducible. But in this case, just right-click on the tables-folder in the left sidebar, and choose import data. The wizard will guide you through the rest. library(dplyr) df &lt;- read.csv(&#39;~/citywide-payroll-data-fiscal-year.csv&#39;, sep=&quot;,&quot;, col.names = c(&#39;YEAR&#39;, &#39;PAYROLL_NO&#39;, &#39;AGENCY_NAME&#39;, &#39;LAST_NAME&#39;, &#39;FIRST_NAME&#39;, &#39;MID_INIT&#39;, &#39;AGENCY_START_DATE&#39;, &#39;WORK_LOCATION_BOROUGH&#39;, &#39;TITLE_DESCRIPTION&#39;, &#39;LEAVE_STATUS_AS_OF_JUNE_30&#39;, &#39;BASE_SALARY&#39;, &#39;PAY_BASIS&#39;, &#39;REGULAR_HOURS&#39;, &#39;REGULAR_GROSS_PAID&#39;, &#39;OT_HOURS&#39;, &#39;TOTAL_OT_PAID&#39;, &#39;TOTAL_OTHER_PAY&#39;), header = 1, stringsAsFactors = F) dbWriteTable(con, &quot;NYCPAY&quot;, df) After the data is imported, we want to do a little bit of data analysis to find out what we are dealing with. We can already see there are a few different ways to measure pay (the PAY BASIS column), which we might have to deal with in order to compare pay. But more importantly, since we want to analyze the carreer development of employees, we need to make sure that we have some sort of way of uniquely identifying each employee. But first, as a matter of good practice, let's add an anonymous ID for each person so that we don't have to use full names everywhere. ALTER TABLE nycpay ADD PERS_ID VARCHAR(32) ; UPDATE nycpay SET PERS_ID = ORA_HASH(FIRST_NAME || MID_INIT || LAST_NAME); COMMIT; It is easier to have one column for identifying a person, and if you have worked with personal data before, you will hopefully appreciate working with data that isn't directly personally identifiable (PII). Since the dataset is public this is in principle an empty gesture, but removing PII is nevertheless valuable as a general practice. Back to uniquely identifying one person. Names are of course not unique, but we also have another challenge: One person may have more than one job with the city. This is a real challenge, because the data won't be unique by person and year - and this is what we want to track. How does each person fare. In practice we have two choices: Aggregate the data so that it is unique per person per year, or simply remove persons who have had multiple jobs. First, let's check how many people actually have multiple jobs: SELECT COUNT(1), COUNT(DISTINCT pers_id), SUM(occ) FROM ( SELECT year, pers_id, COUNT(1) AS occ FROM nycpay GROUP BY year, pers_id HAVING count(1)&gt;1 ) Table 11.3: 1 records COUNT(1) COUNT(DISTINCTPERS_ID) SUM(OCC) 834680 259922 1831579 We can see that out of 3.9 million rows, over 1.8 million belongs to people who have had more than one job in one year. And that there are 259 922 people who have had more than one job. If our goal was to conduct a thorough review of the city's pay data, we would have to choose to aggregate the data in some way in order to construct a dataset that contained every person and a meaningful unique identifier. Further exploration shows that a large number of people are election workers, which we might not find all that interesting to analyze in a career perspective. We use the chance to create a second table without the names, without the people causing the duplicates, and with some other data tidying thrown in. CREATE TABLE SIMPLEPAY AS ( SELECT year, pers_id, TRIM(upper(AGENCY_NAME)) AS agency, TO_DATE(SUBSTR(AGENCY_START_DATE, 1, 10), &#39;YYYY-MM-DD&#39;) AS job_start, TRIM(WORK_LOCATION_BOROUGH) AS job_location, TRIM(TITLE_DESCRIPTION) AS job_title, TRIM(LEAVE_STATUS_AS_OF_JUNE_30) AS leave_status, base_salary, TRIM(pay_basis) AS pay_basis, regular_hours, regular_gross_paid, ot_hours, total_ot_paid, TOTAL_OTHER_PAY FROM nycpay WHERE TRIM(upper(TITLE_DESCRIPTION)) != &#39;ELECTION WORKER&#39; AND pers_id NOT IN( SELECT DISTINCT pers_id FROM nycpay WHERE TRIM(upper(TITLE_DESCRIPTION)) = &#39;ELECTION WORKER&#39; GROUP BY year, pers_id HAVING COUNT(1)&gt;1 ) ); This leaves us with only about 250 000 records (people) per year, but this is hopefully people with fairly regular employment contracts for whom we can do meaningful analysis. Before we go any further, let's check a column that looks promising: Regular gross pay. This seems to always have a value, which might make it a good indicator of what a person actually makes (some of the other pay-related columns have 0-values, which is not a likely wage level. We can use the WIDTH_BUCKET function to create bins, and count the number of people in the respective wage brackets. We create 20 bins between $0 and $200,000, which leaves us with neat $10,000 increments. Any values outside the $0-$200,000 interval will be grouped in &quot;overflow&quot; buckets at the low and high end. SELECT WIDTH_BUCKET(BASE_SALARY, 0, 200000, 20) AS pay, MIN(BASE_SALARY) AS from_pay, MAX(BASE_SALARY) AS to_pay, COUNT(1) AS employee_count FROM simplepay GROUP BY WIDTH_BUCKET(BASE_SALARY, 0, 200000, 20) ORDER BY 1 Table 11.4: Displaying records 1 - 10 PAY FROM_PAY TO_PAY EMPLOYEE_COUNT 1 1e-02 9910 1299806 2 1e+04 19928 2048 3 2e+04 29999 65973 4 3e+04 39999 267240 5 4e+04 49999 318872 6 5e+04 59998 257477 7 6e+04 69999 238067 8 7e+04 79999 303499 9 8e+04 89999 317832 10 9e+04 99999 183706 There are no negative values, but a lot of employees with less than $10,000 in base pay. A quick look at the job titles for these low-paying jobs may reveal something about what type of jobs this is. SELECT job_title, COUNT(1) AS num_employees FROM simplepay WHERE base_salary&lt;10000 GROUP BY job_title ORDER BY 2 DESC FETCH FIRST 20 ROWS ONLY Table 11.5: Displaying records 1 - 10 JOB_TITLE NUM_EMPLOYEES TEACHER- PER SESSION 589323 TEACHER-GENERAL ED 82246 F/T SCHOOL AIDE 57402 JOB TRAINING PARTICIPANT 52810 SUBSTITUTE ED PARA 50308 COLLEGE ASSISTANT 37153 ADJUNCT LECTURER 28939 F/T SCHOOL LUNCH HELPER 28515 STUDENT AIDE 22667 CITY SEASONAL AIDE 22412 The largest group is job traning participants, but a large and diverse group of school aides dominates the list. Many of these roles can be argued have an element of volunteering or civil engagement to them - pay might not be the main reason these people are doing the job. The job training participants are a good testcase: Let's look at how they fared. Did any of them end up with a job in the city? SELECT * FROM ( SELECT year, pers_id, agency, job_title, job_start, base_salary, pay_basis, leave_status FROM simplepay ) MATCH_RECOGNIZE( PARTITION BY pers_id ORDER BY year MEASURES MONTHS_BETWEEN(FIRST(fulljob.job_start), training.job_start) AS time_to_job, COUNT(fulljob.*) AS fulljob_years ALL ROWS PER MATCH PATTERN ( ^ training fulljob+ ) DEFINE training AS job_title=&#39;JOB TRAINING PARTICIPANT&#39;, fulljob AS job_title!=&#39;JOB TRAINING PARTICIPANT&#39; ) Table 11.6: Displaying records 1 - 10 PERS_ID YEAR TIME_TO_JOB FULLJOB_YEARS AGENCY JOB_TITLE JOB_START BASE_SALARY PAY_BASIS LEAVE_STATUS 1000843506 2016 NA 0 DEPT OF PARKS &amp; RECREATION JOB TRAINING PARTICIPANT 2016-05-04 11.8 per Hour ACTIVE 1000843506 2017 14 1 DEPT OF PARKS &amp; RECREATION CITY SEASONAL AIDE 2017-06-30 15.2 per Hour ACTIVE 1002309329 2015 NA 0 DEPT OF PARKS &amp; RECREATION JOB TRAINING PARTICIPANT 2013-04-24 9.4 per Hour CEASED 1002309329 2015 19 1 HRA/DEPT OF SOCIAL SERVICES ELIGIBILITY SPECIALIST 2014-12-08 33284.0 per Annum CEASED 1006085312 2016 NA 0 DEPT OF PARKS &amp; RECREATION JOB TRAINING PARTICIPANT 2015-06-29 11.8 per Hour CEASED 1006085312 2020 49 1 DEPT OF HEALTH/MENTAL HYGIENE COMMUNITY COORDINATOR 2019-07-20 36.6 per Hour ACTIVE 1010330046 2018 NA 0 DEPT OF PARKS &amp; RECREATION JOB TRAINING PARTICIPANT 2017-12-13 13.5 per Hour ACTIVE 1010330046 2019 17 1 DEPT OF PARKS &amp; RECREATION CITY PARK WORKER 2019-05-20 16.1 per Hour ACTIVE 1010330046 2020 17 2 DEPT OF PARKS &amp; RECREATION CITY PARK WORKER 2019-05-20 16.1 per Hour CEASED 1011149826 2015 NA 0 DEPT OF PARKS &amp; RECREATION JOB TRAINING PARTICIPANT 2011-07-05 9.3 per Hour CEASED Here, we have selected people who start out as job training participants, and who (the next time we see them) are regular employees (well, not job training participants anyway). We also measure the number of months from they start job training to they start a paid job, and count the number of rows of regular employment. From here, we can continue our analysis by counting the number of people this is, compare it to the number of people who have been in job training programmes, and look at the time between the job training programme and their job start. A practical way to do this is by removing the ALL ROWS PER MATCH clause, which will leave us with one row per person. SELECT * FROM ( SELECT year, pers_id, agency, job_title, job_start, base_salary, pay_basis, leave_status FROM simplepay ) MATCH_RECOGNIZE( PARTITION BY pers_id ORDER BY year MEASURES MONTHS_BETWEEN(FIRST(fulljob.job_start), FIRST(training.job_start)) AS time_to_job, COUNT(fulljob.*) AS fulljob_years PATTERN ( ^ training+ fulljob+ ) DEFINE training AS job_title=&#39;JOB TRAINING PARTICIPANT&#39;, fulljob AS job_title!=&#39;JOB TRAINING PARTICIPANT&#39; ) Table 11.7: Displaying records 1 - 10 PERS_ID TIME_TO_JOB FULLJOB_YEARS 1000843506 14 1 1002309329 19 1 1002319496 21 4 1004555801 0 3 1006085312 49 1 1006328677 16 2 1010330046 17 2 1011280536 37 2 101235340 77 1 1020333191 0 2 Just by deleting a line, we have an aggregate, with the partition variable (PERS_ID), and the MEASURES we defined. This is a great start to take into R for further analysis. We have also changed the query slightly, opening up for multiple years of job training (the training+ argument), and changed our measure to make sure we get moths between the start date of the first year in job training and the start date of the first job. q &lt;- dbSendQuery(con, &quot; SELECT * FROM ( SELECT year, pers_id, agency, job_title, job_start, base_salary, pay_basis, leave_status FROM simplepay ) MATCH_RECOGNIZE( PARTITION BY pers_id ORDER BY year MEASURES MONTHS_BETWEEN(FIRST(fulljob.job_start), FIRST(training.job_start)) AS time_to_job, COUNT(fulljob.*) AS fulljob_years PATTERN ( ^ training+ fulljob+ ) DEFINE training AS job_title=&#39;JOB TRAINING PARTICIPANT&#39;, fulljob AS job_title!=&#39;JOB TRAINING PARTICIPANT&#39; ) &quot;) df &lt;- fetch(q) library(ggplot2) library(dplyr) df %&gt;% ggplot(aes(TIME_TO_JOB)) + geom_histogram() ## stat_bin: binwidth defaulted to range/30. Use &#39;binwidth = x&#39; to adjust this. A large portion of the people that get a job in the city after job training, seems to be keeping the job they had in job training, but transition to a normal position. For the group of people who didn't get a job straight from their job training, the curve looks roughly linear. The 5 years of data we have corresponds to 60 months, so the non-neglible number of people getting a job close to 100 months is probably due job tranings that started long before 2014 when our data set starts. The long tail of both negative and positive values are most likely a result of data entry mistakes. This is real world data, and there will be real-world errors. "],["inserting-and-updating-data.html", "Chapter 12 Inserting and updating data 12.1 Committing changes to a database 12.2 Truncate, don't drop", " Chapter 12 Inserting and updating data 12.1 Committing changes to a database 12.2 Truncate, don't drop "],["performance-and-integrity.html", "Chapter 13 Performance and integrity 13.1 Indexes 13.2 constraints 13.3 Optimizing your queries", " Chapter 13 Performance and integrity 13.1 Indexes 13.2 constraints 13.3 Optimizing your queries 13.3.1 Inline views 13.3.2 Filter First 13.3.3 Take advantage of partitions 13.3.4 Explain plans 13.3.5 Leave some things for R "],["shiny-and-oracle.html", "Chapter 14 Shiny and Oracle", " Chapter 14 Shiny and Oracle Basically just a question of performance. "],["relevance-to-other-databases.html", "Chapter 15 Relevance to other databases", " Chapter 15 Relevance to other databases When you started learning SQL, you problably thought of SQL as one language. But as you learned more, you might have realized there are subtle differences between different databases. These differences are usually in terms of different functions being available. For example, Oracle has a function named DECODE that works like a brief if-else statement, while SQL Server (Microsoft) has a function named IIF that does basically the same thing. In addition to differences in function names, there are also differences in which features are available. By now, most databases support window-queries (using the OVER statement), but for many years this clause was reserved for more heavy-duty databases. But if you are looking for row pattern matching, you will have to opt for one of only a handfull of databases that supports this. There is a &quot;standard&quot; for the SQL language, defined by the ANSI consortium {https://blog.ansi.org/2018/10/sql-standard-iso-iec-9075-2016-ansi-x3-135/}. For casual users of SQL, reading this standard is probably not very rewarding - especially because this standard is only a guide. Databases are free to leave out some parts, include features that aren't in ANSI, and implement variations that are not entirely in line with the ANSI specification. Still, the ANSI standard serves as a reference, and creates an expectation as to what SQL should do. If you are writing SQL code that is meant to last, you might have to give a thought to wether or not to use functions particular to the DB you happen to use, or stick to the parts of SQL that works across most RDBMSes. Personally I am of the opinion that the benefit of the custom SQL functionality outweighs the drawback of having code tied to a particular product. Although the interwebs tells stories of clean shifts from one DBMS to another, I have not myself seen any of these shifts that did not also come with a major code rewrite as well. When you are juggeling R and SQL, you also have the option to do as much of possible in R and keep the SQL simple - or do a lot of computation in SQL and leave R for the last mile of computing (possibly just creating a visualization). I don't thinks it is useful to speculate in expected lifetime of languages and DBMSes. Instead, write SQL where SQL is more effective for you, and write R where R is more effective for you. "],["big-data-databases.html", "Chapter 16 Big Data Databases 16.1 What is hive, and how does it compare", " Chapter 16 Big Data Databases 16.1 What is hive, and how does it compare "],["what-oracle-r-enterprise-is.html", "Chapter 17 What Oracle R Enterprise is 17.1 Loading ORE 17.2 Pushing R computation to the database 17.3 ORE build-in algorithms 17.4 Calling R from SQL", " Chapter 17 What Oracle R Enterprise is Oracle R Enterprise (ORE) is a collection of utilities that let you write and run R code in the database. Until recently this was an expensive option, but starting in 2020 no licensing is needed. ORE consists of Oracle's own R distribution (which is lagging behind the official R version), an R library (also named ORE) which gives easy integration with database resources like tables and more, and some new PL/SQL functions that let you create, invoke and return the output from R functions. I will try to give examples of two different ways of using ORE: 1. Using ORE from R, as a way to push statistical computations to the database 2. Calling R from SQL, enabling you to return an R dataframe from a SELECT statement, or simply creating a view that returns the result of an R function. 17.1 Loading ORE We will take a shortcut here, and use the Oracle BigData Lite virtual machine. This allows us to skip a lot of installation and configuration, focusing on using the library. Installing and loading ORE is simple and unsurprising: #install.packages(&quot;ORE&quot;) library(ORE) ore.connect(user = &quot;moviedemo&quot;, sid = &quot;orcl&quot;, host = &quot;localhost&quot;, password = &quot;welcome1&quot;, port = 1521, all=TRUE) ## Warning: table &quot;MOVIEDEMO&quot;.&quot;CUSTOMER&quot; contains unsupported data types ## Warning: table &quot;MOVIEDEMO&quot;.&quot;PROSPECTS&quot; contains unsupported data types ## Warning: table &quot;MOVIEDEMO&quot;.&quot;CUSTOMER_RFM&quot; contains unsupported data types ## Warning: table &quot;MOVIEDEMO&quot;.&quot;IRIS_PREDICT&quot; contains unsupported data types The arguments needed to connect to the database are mostly unsurprising, but it is worth noting that ORE can connect to both oracle and Hive (oracle is the default), and there is no default port number - which is why we needed to specify it with the port argument. The all=TRUE argument specifies to load a reference to all database objects. This is nice when your schema is relatively small, but comes at a performance cost if your schema is large. After connecting, database tables and views should be directly accessible from R. You can test this by running head(CUSTOMER_SEGMENT) ## SEGMENT_ID NAME DESCRIPTION ## 1 1 Segment1 Young People - AGE &lt;= 21 ## 2 2 Segment2 &quot;DINKS - Double Income No Kids - 21&lt;AGE&lt;40, Married, Household_size=2&quot; ## 3 3 Segment3 &quot;Married with Children - 21&lt;AGE&lt;40, Married, HHS&gt;2&quot; ## 4 4 Segment4 &quot;Single Male - 21&lt;AGE&lt;40, Single, Male&quot; ## 5 5 Segment5 &quot;Single Female - 21&lt;AGE&lt;40, Single, Female&quot; ## 6 6 Segment6 &quot;Midage Male - 40&lt;=AGE&lt;55, Male&quot; CUSTOMER_SEGMENT is a table in the moviedemo scema that we connected with. 17.2 Pushing R computation to the database 17.3 ORE build-in algorithms ORE comes with a number of different machine-learning algorithms (although nothing fancy). You will probably struggle to find anything ORE can do that R can't, but the library pushes computation to the database - which can give a performance improvement. One such algorithm is KMeans, which assigns each observation to a cluster. A common example is customer segmentation. You have a number of customers, and you may have some basic information about them like their age, gender, location, and purchasing history. Now, you want to send an email to each, advertising newly arrived products. A modern data scientist may see a supervised machine learning model as a solution, but a less technical way of doing this is to create assign each customer to a group, figure out some commonalities in each group, and send each group an email focusing on some of the To illustrate the intuitive usefulness of clustering, take a look at how the iris dataset is distributed: plot(iris$Sepal.Length, iris$Petal.Length, col=iris$Species) The color coding represents species, and it is obvious that plants of the same species tend to have similar lengths for sepals and petals. For the sake of variation, we will demonstrate OREs K-means implementation using the CUSTOMER_V table. head(CUSTOMER_V) ## Warning: ORE object has no unique key - using random order ## Warning: ORE object has no unique key - using random order ## CUST_ID LAST_NAME FIRST_NAME STREET_ADDRESS POSTAL_CODE CITY_ID CITY STATE_PROVINCE_ID STATE_PROVINCE COUNTRY_ID ## 1 1e+07 Re**** Laurel 61 LONDON ROAD PO8 8 52033 COWPLAIN 1113 &lt;NA&gt; 129 ## 2 1e+07 Su**** Otto &lt;NA&gt; &lt;NA&gt; 49239 Kitanoshita 1112 Tokyo 128 ## 3 1e+07 Kl******** Ta-Heng FAIRFAX STREET CV1 5 55742 COVENTRY 1113 &lt;NA&gt; 129 ## 4 1e+07 Ro**** Bhadrabuja 27 DUNDERG ROAD BT51 4 37386 MACOSQUIN 1113 &lt;NA&gt; 129 ## 5 1e+07 Ly***** Candrin 1 COMMERCIAL ROAD EX7 9 110601 DAWLISH 1113 &lt;NA&gt; 129 ## 6 1e+07 Sr****** Motilal 12 ANTONY ROAD PL11 2 89177 TORPOINT 1113 &lt;NA&gt; 129 ## COUNTRY CONTINENT_ID CONTINENT AGE COMMUTE_DISTANCE CREDIT_BALANCE EDUCATION EMAIL FULL_TIME GENDER ## 1 United Kingdom 14 Europe 30 1 21 Bachelors Laurel.Recker@oraclemail.com Yes Male ## 2 Japan 16 Asia 27 40 206 Masters Otto.Suzuki@oraclemail.com Yes Male ## 3 United Kingdom 14 Europe 44 6 136 High School Ta-Heng.Klossovsky@oraclemail.com Yes Male ## 4 United Kingdom 14 Europe 65 18 66 Bachelors Bhadrabuja.Rovigo@oraclemail.com Yes Male ## 5 United Kingdom 14 Europe 49 21 36 Bachelors Candrin.Lyakhov@oraclemail.com Yes Male ## 6 United Kingdom 14 Europe 50 16 165 Bachelors Motilal.Sreedhar@oraclemail.com Yes Male ## HOUSEHOLD_SIZE INCOME INCOME_LEVEL INSUFF_FUNDS_INCIDENTS JOB_TYPE LATE_MORT_RENT_PMTS MARITAL_STATUS MORTGAGE_AMT NUM_CARS ## 1 1 108625 E: 90,000 - 109,999 0 Services 0 S 190000 1 ## 2 1 93770 E: 90,000 - 109,999 1 Professional 1 S 0 1 ## 3 1 11851 A: Below 30,000 1 Business 1 S 140000 1 ## 4 2 54651 C: 50,000 - 69,999 1 Business 0 M 6202 2 ## 5 2 118261 F: 110,000 - 129,999 0 Business 0 M 10000 1 ## 6 1 116367 F: 110,000 - 129,999 0 Business 0 S 0 1 ## NUM_MORTGAGES PET PROMOTION_RESPONSE RENT_OWN SEG WORK_EXPERIENCE YRS_CURRENT_EMPLOYER YRS_CUSTOMER YRS_RESIDENCE COUNTRY_CODE ## 1 1 Dog 0 Own 4 2 2 4 5 GB ## 2 0 Unknown 0 Rent 4 8 2 5 2 JP ## 3 1 Unknown 0 Own 6 18 3 4 16 GB ## 4 1 Dog 0 Own 8 28 20 2 2 GB ## 5 1 Unknown 0 Own 6 25 4 1 1 GB ## 6 0 Unknown 0 Rent 6 28 6 1 1 GB feature_list &lt;- c(&quot;INCOME&quot;, &quot;MORTGAGE_AMT&quot;, &quot;NUM_CARS&quot;, &quot;NUM_MORTGAGES&quot;, &quot;WORK_EXPERIENCE&quot;, &quot;YRS_CURRENT_EMPLOYER&quot;, &quot;YRS_CUSTOMER&quot;, &quot;YRS_RESIDENCE&quot;, &quot;LATE_MORT_RENT_PMTS&quot;, &quot;INSUFF_FUNDS_INCIDENTS&quot;) kmmodel &lt;- ore.odmKMeans(~., CUSTOMER_V[, feature_list] , num.centers=3, iterations = 10) The ore.odmKMeans function takes the formula as the first argument and dataset as the second argument, followed by the desired number of clusters (3) and the number of iterations the algorithm should do (10). The resulting model can be used to associate new observations with existing clusters, or we can simply take a note of the cluster centers available as a dataframe in kmmodel$clusters. 17.4 Calling R from SQL Oracle reuses the concept of scripts when interacting with R from SQL, allowing you to register a &quot;script&quot; that is basically an R function that takes input and returns output like any other function. The challenge and potential for confusion lies in mapping this input and output between SQL and R. In our example, we return to the iris dataset (yawn), and create a prediction model that we can access from SQL and even represent as a view for convenience. In order for this to be a workable end-to-end example, we have to start with a little data preparation, including writing the iris dataset to the DB. If you haven't already, remember to connect to the database using the ore.connect function. # making iris visible, and DB-izing variable names iris_to_db &lt;- iris names(iris_to_db) &lt;- c(&quot;SEPAL_LENGTH&quot;, &quot;SEPAL_WIDTH&quot;, &quot;PETAL_LENGTH&quot;, &quot;PETAL_WIDTH&quot;, &quot;SPECIES&quot;) if (ore.exists(&quot;IRIS_RC&quot;)) { ore.drop(&quot;IRIS_RC&quot;) } ore.create(iris_to_db, table=&quot;IRIS_RC&quot;) OK, so we have renamed some column names for convenience, and created a table IRIS_R in the DB. I might not have shown the ore.create function before, but it does what it suggest - it creates a table in the db. Other than that one command though, we are not seeing anything new so far. That is about to change. Our ultimate goal is to be able to call on a prediction model from Oracle, but in order to do that we need a place to store the model - a place Oracle can access. Oracle has created such an &quot;object storage&quot; for R, and we can save objects there through the ore.save command. Let's try creating the model we want and saving it. iris_species_model &lt;- ore.odmNB(SPECIES ~ SEPAL_WIDTH + PETAL_LENGTH + PETAL_WIDTH + SEPAL_LENGTH, IRIS_RC) ore.save(iris_species_model, name=&#39;supervised_models&#39;, list=&#39;iris_species_model&#39;, overwrite = TRUE) The name argument specifies the environment we want to save the model to - you can basically think of it as a folder, and the list argument is a list (ahem vector ahem) of the names you want to assign. If you are saving multiple elements, you can pass multiple objects before the named arguments. Make sure the number of objects and length of the list matches, and make sure the order is correct. We have now created an R model and assigned it to the R-variable iris_species_model, as one typically does in R. Next, we saved this model to Oracle, using the name iris_species_model. In summary, we now have the same model, with the same name, both in R and in Oracle. The difference is that the model in Oracle is persistent, while the model in R disappears when you shut down R (granted R does a good job at saving the R environment for you, but you get the point). Now, we are halfway - but this is the most difficult part: We need to register a script (function) that gives us the predictions from SQL. We haven't talked about the details of what we want yet - what exactly is the output we want, and what is the input the model needs? There are many ways to skin this cat and our chosen approach might not be the most efficient one, but it shows some of the hoops you need to jump through. We are passing the function an entire dataframe, together with a reference to the model we are using. The function returns a dataframe with the predictions. iris_species_predict &lt;- function(dat, ds, obj) { ore.load(name=ds, list=obj) mod &lt;- get(obj) dat &lt;- ore.frame(dat) prd &lt;- predict(mod, dat,supplemental.cols=&quot;SPECIES&quot;) ore.pull(prd) } Our function has to work within some constraints. The dataset that is being passed has to originate from a table in the database, and we use ore.frame to convert the input dataset to a &quot;proper&quot; dataframe that R can work with. In order to load the model we saved (named iris_species_model), we need a reference both to the environment in which the model is stored and the name of the model. the ore.load. The ore.load command largely mirrors the native load command. Once this is done, we can use the model we loaded to predict the species we want, and return it as a dataframe. So far so good. It's an R function, there is a little bit of legwork involved in loading the model and handling the incoming data from SQL, but mostly it is recognizible. And we can save this function to Oracle as a script. ore.scriptCreate(&#39;iris_species_predict&#39;, iris_species_predict, overwrite = TRUE) To recap, we have: 1. Trained a model that predicts iris species. 2. Saved that model to Oracle, so that it is accessible to our moviedemo database user and can be loaded any time. 3. Created a function that uses the model to predict species given an incoming dataset 4. registered this function as a script in Oracle. We have now done what we can in R, the next step is to bring this together from SQL. In SQL, we can call this script using the rqTableEval function. The input to this function is fairly involved, not to mention unintuitive. We ha SELECT * FROM table(rqTableEval( cursor(SELECT * FROM IRIS_RC), cursor(SELECT &#39;supervised_models&#39; as &quot;ds&quot;, &#39;iris_species_model&#39; as &quot;obj&quot;, 1 as &quot;ore.connect&quot; FROM dual), &#39;SELECT 1 AS SETOSA_PROB, 1 AS VERSICOLOR_PROB, 1 AS VIRGINICA_PROB, CAST(&#39;&#39;a&#39;&#39; AS VARCHAR(255)) AS SPECIES, CAST(&#39;&#39;b&#39;&#39; AS VARCHAR(255)) AS PREDICTED_SPECIES FROM dual&#39;, &#39;iris_species_predict&#39;) ) WHERE species!=predicted_species Table 17.1: 8 records SETOSA_PROB VERSICOLOR_PROB VIRGINICA_PROB SPECIES PREDICTED_SPECIES 0 0.24 0.76 versicolor virginica 0 0.00 1.00 versicolor virginica 0 0.44 0.56 versicolor virginica 0 0.44 0.56 versicolor virginica 0 0.24 0.76 versicolor virginica 0 1.00 0.00 virginica versicolor 0 0.84 0.16 virginica versicolor 0 0.84 0.16 virginica versicolor We are constructing a table (as indicating by the table function that wraps it all), inside it is the rqTableEval function that takes three arguments (not the same three as the R function, mind you...). In order, the inputs are: The input dataset, in the form of a SELECT statement from the IRIS_R table we created. If we cared about the model itself we would hav split the data into a train and test set, but we are simply showing the technical bits for now. All wrapped in a cursor. The additional arguments, ds and obj, in the form of simple strings selected from dual. Again, inside a cursor. A mock SELECT statement indicating the column types of the return dataset, formatted as a long string. This is unintuitive at best and it seems weird that Oracle can talk to R about everything except the structure of the returning data frame, but apparently that is where we are at. In detail we are expecting one numerical column with a probability for each of the species, so we select 1 (as a stand-in for a number) and name it for each of the three species. We also expect two columns, the actual and predicted species, as text columns. For text we need to make sure the column is big enough, so we select some letter and explicitly cast it as VARCHAR(255) which we know will be more than enough for the species name. This query can now be run, and you should get in return the same dataset you get when you call predict directly in R. Congratulations. Save for the illegible syntax, you have now seamlessly combined R and SQL. "],["references.html", "References", " References "]]
